# Website HTML Chunk Collection

This folder contains small chunks of HTML extracted from various websites related to animal rights, veganism, and plant-based diets. These HTML chunks are structured in a way that they can be easily processed by embedding models. For each URL, the entire HTML is broken down into smaller parts, with each chunk representing a section of the original webpage. Together, these chunks make up the full HTML for that website.

## Project and License

The HTML chunks in this collection have been sourced from a variety of online publications, blogs, and media platforms, primarily from organizations and businesses advocating for animal rights. All website owners have agreed to share their data through signed agreements, ensuring ethical usage of this data in our database. The HTML chunks are freely available for use in AI & ML applications.

## CSV Structure

The CSV files contain the following columns:

- **URL**: The URL of the original webpage.
- **Chunk**: A small portion of the HTML extracted from the webpage.
- **Type**: The type of the chunk (e.g., HTML, image).
- **Position**: The position of the chunk within the original webpage.

### Example CSV Structure

```html
url, chunk, type, position
https://www.openpaws.ai/research-and-reports/literature-review-on-developing-artificial-intelligence-to-advocate-for-animal-rights, <div class="content"><h1>Literature Review on AI for Animal Rights</h1><p>...</p></div>, html, 1

This structure ensures that all chunks of HTML are categorized and labeled effectively for use in AI & ML applications.

## Website Scraping Script

To facilitate the generation of similar datasets from any website, we provide the following Google Colab notebooks:

- `website_html_scraping.ipynb`: Scrapes content from an entire website by crawling internal links and extracting HTML.

The scripts organize the extracted HTML chunks into the structured CSV format described above.

## Features of the Script

- **HTML Extraction**: Extracts HTML and content from specified websites.
- **HTML Chunking**: Breaks down HTML into smaller, manageable chunks for embedding models.
- **CSV Output**: Outputs the HTML chunks into a CSV file with proper labeling and structure.

## How to Use the Script

1. **Open Google Colab**: Go to Google Colab.
2. **Upload the Script**: Upload the `website_content_scraping.ipynb` file to your Google Colab environment.
3. **Configure**: Update the script with the URLs of the websites you wish to scrape.
4. **Run**: Execute the cells in the notebook to scrape the content and generate the CSV file.

By using this script, you can expand the dataset with additional HTML chunks from other relevant websites, maintaining the consistency and structure needed for AI & ML applications.
