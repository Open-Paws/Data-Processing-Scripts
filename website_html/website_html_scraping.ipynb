{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KIDFiLoBGsY7"
      },
      "outputs": [],
      "source": [
        "# Install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install selenium requests beautifulsoup4 webdriver-manager google.colab\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "import urllib.parse\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager  # Use webdriver-manager to install the correct version of ChromeDriver\n",
        "from selenium.webdriver.common.by import By\n",
        "\n",
        "# Your 2Captcha API key\n",
        "API_KEY = \"YOUR_API_KEY_HERE\"  # Replace with your actual 2Captcha API key\n",
        "\n",
        "# Initialize Selenium WebDriver (headless Chrome) with ChromeDriverManager\n",
        "def init_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # Automatically download and set up ChromeDriver with ChromeDriverManager\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to solve CAPTCHA using 2Captcha\n",
        "def solve_captcha(driver, site_key, url):\n",
        "    if not site_key:\n",
        "        print(\"No site key found, skipping CAPTCHA solving.\")\n",
        "        return None\n",
        "\n",
        "    # Send CAPTCHA request to 2Captcha\n",
        "    captcha_id_response = requests.post(\n",
        "        \"http://2captcha.com/in.php\",\n",
        "        data={\n",
        "            'key': API_KEY,\n",
        "            'method': 'userrecaptcha',\n",
        "            'googlekey': site_key,\n",
        "            'pageurl': url,\n",
        "            'json': 1\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    if captcha_id_response[\"status\"] == 1:\n",
        "        captcha_id = captcha_id_response[\"request\"]\n",
        "        print(\"CAPTCHA sent for solving...\")\n",
        "        # Poll for solution\n",
        "        fetch_url = f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "        while True:\n",
        "            time.sleep(5)  # Delay before each poll\n",
        "            response = requests.get(fetch_url).json()\n",
        "            if response[\"status\"] == 1:\n",
        "                print(\"CAPTCHA solved\")\n",
        "                return response[\"request\"]\n",
        "            elif response[\"status\"] == 0 and response[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "                print(\"Waiting for CAPTCHA to be solved...\")\n",
        "            else:\n",
        "                print(\"Error solving CAPTCHA:\", response)\n",
        "                return None\n",
        "    else:\n",
        "        print(\"Error submitting CAPTCHA:\", captcha_id_response)\n",
        "        return None\n",
        "\n",
        "# Function to extract the site_key automatically\n",
        "def get_site_key(driver):\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    # Look for 'data-sitekey' attributes in the page\n",
        "    site_key = None\n",
        "    recaptcha_div = soup.find('div', {'class': 'g-recaptcha', 'data-sitekey': True})\n",
        "    if recaptcha_div:\n",
        "        site_key = recaptcha_div['data-sitekey']\n",
        "    else:\n",
        "        # Try to find the key in a script tag if not in div\n",
        "        script_tags = soup.find_all('script')\n",
        "        for script in script_tags:\n",
        "            if 'data-sitekey' in str(script):\n",
        "                match = re.search(r'data-sitekey=\"(.+?)\"', str(script))\n",
        "                if match:\n",
        "                    site_key = match.group(1)\n",
        "                    break\n",
        "\n",
        "    if site_key:\n",
        "        print(f\"Found site key: {site_key}\")\n",
        "    else:\n",
        "        print(\"Site key not found\")\n",
        "    return site_key\n",
        "\n",
        "# Function to clean URLs\n",
        "def clean_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    clean_path = re.sub(r'/+', '/', parsed_url.path)  # Remove duplicate slashes\n",
        "    clean_url = urllib.parse.urlunparse(parsed_url._replace(path=clean_path, query='', fragment=''))\n",
        "    return clean_url\n",
        "\n",
        "# HTML chunking function without breaking tags\n",
        "def split_html_into_chunks(html, max_chunk_size=2000):\n",
        "    chunks = []\n",
        "    current_chunk = ''\n",
        "    position = 0\n",
        "\n",
        "    # Find all positions where we can safely split (after closing tags)\n",
        "    split_positions = [m.end() for m in re.finditer(r'>', html)]\n",
        "    split_positions.append(len(html))  # Ensure we include the end of the HTML\n",
        "\n",
        "    last_position = 0\n",
        "    for pos in split_positions:\n",
        "        # Extract potential chunk\n",
        "        potential_chunk = html[last_position:pos]\n",
        "        if len(current_chunk) + len(potential_chunk) <= max_chunk_size:\n",
        "            current_chunk += potential_chunk\n",
        "            last_position = pos\n",
        "        else:\n",
        "            # Add current chunk to chunks and start a new chunk\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = potential_chunk\n",
        "            last_position = pos\n",
        "\n",
        "    # Add any remaining content as the last chunk\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to extract raw HTML content from a webpage\n",
        "def extract_content(driver, url):\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Automatically find the site_key for CAPTCHA\n",
        "    site_key = get_site_key(driver)\n",
        "\n",
        "    # If CAPTCHA detected, solve it\n",
        "    if site_key:\n",
        "        captcha_solution = solve_captcha(driver, site_key, url)\n",
        "        if captcha_solution:\n",
        "            driver.execute_script(f'document.getElementById(\"g-recaptcha-response\").innerHTML=\"{captcha_solution}\";')\n",
        "            driver.execute_script('document.querySelector(\"form\").submit();')\n",
        "            time.sleep(5)\n",
        "            driver.get(url)\n",
        "\n",
        "    # Get the raw HTML content\n",
        "    html_content = driver.page_source\n",
        "\n",
        "    # Split the HTML into chunks without breaking tags\n",
        "    chunks = split_html_into_chunks(html_content, max_chunk_size=2000)\n",
        "\n",
        "    content = []\n",
        "    position = 1\n",
        "    for chunk in chunks:\n",
        "        content.append((url, chunk, 'HTML', position))\n",
        "        position += 1\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to get all internal links from a webpage\n",
        "def get_all_links(driver, url, domain):\n",
        "    links = set()\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            href = urllib.parse.urljoin(url, href)\n",
        "            parsed_href = urllib.parse.urlparse(href)\n",
        "            if parsed_href.netloc == domain:\n",
        "                clean_href = clean_url(href)\n",
        "                links.add(clean_href)\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting links from {url}: {e}\")\n",
        "    return links\n",
        "\n",
        "# Function to crawl the website and collect all URLs\n",
        "def crawl_website(driver, base_url):\n",
        "    domain = urllib.parse.urlparse(base_url).netloc\n",
        "    to_visit = set([base_url])\n",
        "    visited = set()\n",
        "    all_urls = []\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "        print(f\"Crawling: {current_url}\")\n",
        "        visited.add(current_url)\n",
        "        all_urls.append(current_url)\n",
        "\n",
        "        # Get all internal links from the current page\n",
        "        links = get_all_links(driver, current_url, domain)\n",
        "        to_visit.update(links - visited)\n",
        "\n",
        "    return all_urls\n",
        "\n",
        "# Function to scrape all content from a list of URLs\n",
        "def scrape_all_content(driver, urls):\n",
        "    all_content = []\n",
        "    for url in urls:\n",
        "        url = clean_url(url)\n",
        "        print(f\"Scraping content from: {url}\")\n",
        "\n",
        "        try:\n",
        "            content = extract_content(driver, url)\n",
        "            all_content.extend(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(2)  # Wait for 2 seconds between requests\n",
        "\n",
        "    return all_content\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL of the website to crawl\n",
        "    base_url = \"https://WEBSITE_YOU_WANT_TO_CRAWL.com\"  # Replace with your target website\n",
        "\n",
        "    # Initialize the driver\n",
        "    driver = init_driver()\n",
        "\n",
        "    # Crawl the website to get all URLs\n",
        "    all_urls = crawl_website(driver, base_url)\n",
        "    print(f\"Total URLs collected: {len(all_urls)}\")\n",
        "\n",
        "    # Scrape content from the collected URLs\n",
        "    all_content = scrape_all_content(driver, all_urls)\n",
        "\n",
        "    # Export to CSV\n",
        "    csv_filename = 'FILE_NAME.csv' # Replace with the name you want the exported file to have\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
        "        writer.writerow(['url', 'content', 'type', 'position'])  # Header row\n",
        "        for row in all_content:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Website content has been saved to {csv_filename}\")\n",
        "\n",
        "    # Download the CSV file to the local device\n",
        "    files.download(csv_filename)\n",
        "\n",
        "    # Clean up and close the driver\n",
        "    driver.quit()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}