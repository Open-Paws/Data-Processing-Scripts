{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UACWlvqlQ6Jn"
      },
      "outputs": [],
      "source": [
        "# Install and import necessary libraries - Hugging Face datasets, Google Cloud AI Platform, Google Auth, and Requests\n",
        "!pip install datasets google-cloud-aiplatform google-auth requests\n",
        "\n",
        "import json\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.generative_models import GenerativeModel, Content, Part, GenerationConfig\n",
        "from google.colab import files\n",
        "\n",
        "# Authenticate to Google Cloud - this will open a new tab to authenticate\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up your Google Cloud project and location - replace with your actual project ID and location\n",
        "PROJECT_ID = 'PROJECT_ID'  # Replace with your actual project ID - e.g., 'my-project-id'\n",
        "LOCATION = 'LOCATION'    # Replace with the Google Cloud region you want to use - e.g., 'us-central1'\n",
        "\n",
        "# Initialize Vertex AI with the specified project and location - this will set the default project and location for Vertex AI\n",
        "print(f\"Initializing Vertex AI for project {PROJECT_ID} in location {LOCATION}...\")\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Define the model ID for the Gemini model to be used for generating content - replace with your actual model ID\n",
        "MODEL_ID = 'gemini-1.5-flash-001'\n",
        "\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract JSON object from the response text.\n",
        "\n",
        "    Args:\n",
        "    response_text (str): The response text from which to extract JSON.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: The extracted JSON object or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            return json.loads(match.group(0))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {response_text}\")\n",
        "    return None\n",
        "\n",
        "def rank_message(message_text, language):\n",
        "    \"\"\"\n",
        "    Rank a message based on the CRITERIA scale using the Gemini model.\n",
        "\n",
        "    Args:\n",
        "    message_text (str): The text of the message to be ranked.\n",
        "    language (str): The language of the message.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: A dictionary containing CRITERIA scores or None if ranking fails.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You will be tasked with evaluating content based on the CRITERIA scale. For each piece of content provided, you will generate scores for the following eight criteria, each on a scale from 0 to 1. Output the results in JSON format.\n",
        "\n",
        "    ### Criteria\n",
        "\n",
        "    1. **Cultural Sensitivity**: Measure how culturally inclusive the content is.\n",
        "       - **Culturally Inclusive (0.8-1.0)**: The content shows respect for diverse cultural perspectives and uses culturally sensitive approaches to animal advocacy.\n",
        "       - **Moderately Inclusive (0.4-0.7)**: The content generally respects cultural diversity but may lack depth in cultural sensitivity.\n",
        "       - **Culturally Insensitive (0.0-0.3)**: The content lacks respect for cultural diversity and fails to use culturally sensitive approaches.\n",
        "    2. **Relevance**: Measure how pertinent the content is to veganism and animal rights.\n",
        "       - **Highly Relevant (0.8-1.0)**: The content is directly related to veganism, animal rights, vegan lifestyle, plant-based diets, animal welfare, ethical treatment of animals, or advocacy for animal rights.\n",
        "       - **Moderately Relevant (0.4-0.7)**: The content indirectly relates to veganism and animal rights through broader ethical, dietary, or sustainability discussions.\n",
        "       - **Not Relevant (0.0-0.3)**: The content is unrelated to veganism or animal rights.\n",
        "    3. **Insight**: Judge the level of insight provided by the key concept in the content.\n",
        "       - **Highly Insightful (0.8-1.0)**: The content provides deep, original insights that significantly advance the understanding of veganism or animal advocacy.\n",
        "       - **Moderately Insightful (0.4-0.7)**: The content offers useful insights that enhance understanding but may not be particularly original.\n",
        "       - **No Unique Insights (0.0-0.3)**: The content provides no meaningful insights or repeats well-known information.\n",
        "    4. **Trustworthiness**: Rate the accuracy, reliability, and credibility of the information presented.\n",
        "       - **Highly Trustworthy (0.8-1.0)**: The information is accurate, well-researched, and comes from credible sources.\n",
        "       - **Moderately Trustworthy (0.4-0.7)**: The information is generally accurate but may include some minor errors or questionable sources.\n",
        "       - **Untrustworthy (0.0-0.3)**: The information is inaccurate, misleading, or based on non-credible sources.\n",
        "    5. **Emotional Impact**: Measure the emotional engagement the content provides.\n",
        "       - **Very Emotionally Impactful (0.8-1.0)**: The content effectively elicits empathy and emotional engagement.\n",
        "       - **Moderately Emotionally Impactful (0.4-0.7)**: The content elicits some emotional engagement but may lack depth.\n",
        "       - **Not Emotionally Impactful (0.0-0.3)**: The content fails to elicit any emotional response.\n",
        "    6. **Rationality**: Evaluate the logical consistency and reasoning in the content.\n",
        "       - **Very Rational (0.8-1.0)**: The content is logically consistent, well-reasoned, and supported by evidence.\n",
        "       - **Moderately Rational (0.4-0.7)**: The content is generally rational but may contain some logical inconsistencies or weak arguments.\n",
        "       - **Not Rational (0.0-0.3)**: The content lacks logical consistency and sound reasoning.\n",
        "    7. **Influence**: Assess the potential of the content to encourage actions and lifestyle changes.\n",
        "       - **Highly Likely to Influence Behavior (0.8-1.0)**: The content has strong potential to encourage actions and lifestyle changes.\n",
        "       - **Moderately Likely to Influence Behavior (0.4-0.7)**: The content has some potential to influence behavior but may not be compelling enough to drive significant changes.\n",
        "       - **Not Likely to Influence Behavior (0.0-0.3)**: The content is unlikely to influence any behavior change.\n",
        "    8. **Alignment**: Assess how well the content aligns with vegan and animal rights ethics.\n",
        "       - **Highly Aligned (0.8-1.0)**: The content strongly aligns with the ethical principles and core values of veganism and animal rights.\n",
        "       - **Moderately Aligned (0.4-0.7)**: The content supports some aspects of vegan ethics but may include neutral or slightly contradictory elements.\n",
        "       - **Not Aligned (0.0-0.3)**: The content contradicts or is indifferent to vegan principles.\n",
        "\n",
        "    ### JSON Output Format\n",
        "\n",
        "    For each piece of content, output a JSON object with the following structure:\n",
        "\n",
        "    {{\n",
        "      \"CRITERIA_scores\": {{\n",
        "        \"Cultural_Sensitivity\": <score_from_0_to_1>,\n",
        "        \"Relevance\": <score_from_0_to_1>,\n",
        "        \"Insight\": <score_from_0_to_1>,\n",
        "        \"Trustworthiness\": <score_from_0_to_1>,\n",
        "        \"Emotional_Impact\": <score_from_0_to_1>,\n",
        "        \"Rationality\": <score_from_0_to_1>,\n",
        "        \"Influence\": <score_from_0_to_1>,\n",
        "        \"Alignment\": <score_from_0_to_1>\n",
        "      }},\n",
        "      \"CRITERIA_final_score\": <average_of_all_scores>,\n",
        "      \"language\": \"{language}\"\n",
        "    }}\n",
        "\n",
        "    Content:\n",
        "    {message_text}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Create the GenerativeModel object - this will load the model for generating content\n",
        "        gemini_model = GenerativeModel(model_name=MODEL_ID)\n",
        "\n",
        "        # Set up the generation configuration with parameters controlling the output - adjust as needed\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=0.7,          # Controls the randomness of the output - higher values make the output more random\n",
        "            max_output_tokens=512,    # Maximum number of tokens in the output - adjust based on the model's maximum output length\n",
        "            top_p=0.9,                # Top-p (nucleus) sampling parameter - higher values make the output more diverse\n",
        "            top_k=40                  # Top-k sampling parameter - higher values make the output less random\n",
        "        )\n",
        "\n",
        "        # Generate the content using the model - this will rank the message based on the CRITERIA scale\n",
        "        print(\"Generating CRITERIA scores for the message...\")\n",
        "        response = gemini_model.generate_content(\n",
        "            contents=[Content(role=\"user\", parts=[Part.from_text(prompt)])],\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        # Extract and parse the JSON response - this will extract the CRITERIA scores from the generated content\n",
        "        criteria_scores = extract_json_from_response(response.text)\n",
        "        if criteria_scores:\n",
        "            return criteria_scores\n",
        "        else:\n",
        "            print(\"No CRITERIA scores found in the response.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during message ranking: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_subset(subset_name, output_file, max_messages, start_message):\n",
        "    \"\"\"\n",
        "    Process a subset of the Hugging Face dataset, rank messages, and save the results to the output file.\n",
        "\n",
        "    Args:\n",
        "    subset_name (str): Name of the subset.\n",
        "    output_file (str): Path to the output JSONL file.\n",
        "    max_messages (int): Maximum number of messages to process.\n",
        "    start_message (int): Starting message number for processing.\n",
        "    \"\"\"\n",
        "    print(f\"Loading subset: {subset_name}\")\n",
        "    dataset = load_dataset('Cohere/wikipedia-22-12', subset_name, split='train')\n",
        "\n",
        "    message_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(output_file, 'a', encoding='utf-8') as outfile:\n",
        "            for idx, item in enumerate(dataset):\n",
        "                if message_count >= max_messages:\n",
        "                    print(\"Reached the maximum number of messages to process.\")\n",
        "                    break\n",
        "\n",
        "                if idx < start_message - 1:\n",
        "                    continue\n",
        "\n",
        "                print(f\"Processing message {start_message + message_count}...\")\n",
        "\n",
        "                # Add this snippet\n",
        "                if subset_name == 'simple':\n",
        "                    language = 'en'\n",
        "                else:\n",
        "                    language = subset_name\n",
        "\n",
        "                criteria_scores = rank_message(item['text'], language)\n",
        "                if criteria_scores is not None:\n",
        "                    item['CRITERIA'] = criteria_scores\n",
        "                    json.dump(item, outfile, ensure_ascii=False)\n",
        "                    outfile.write('\\n')\n",
        "                    print(f\"CRITERIA scores added to message {start_message + message_count}.\")\n",
        "                else:\n",
        "                    print(f\"Failed to add CRITERIA scores to message {start_message + message_count}.\")\n",
        "\n",
        "                message_count += 1\n",
        "\n",
        "            # Final save and download at the end of processing - this will save the final progress\n",
        "            print(\"Final save...\")\n",
        "            files.download(output_file)\n",
        "            print(f\"File {output_file} downloaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Set definitions for all variables - adjust as needed\n",
        "# Possible subset names: 'ar', 'de', 'en', 'es', 'fr', 'hi', 'it', 'ja', 'ko', 'simple', 'zh'\n",
        "subset_name = 'simple'  # Specify the subset to process (e.g., 'ar', 'de', 'en', etc.)\n",
        "output_file = f'wiki_simple.jsonl'  # Path to the output JSONL file - replace with the name you want for the output file\n",
        "max_messages = 5000  # Set the maximum number of messages to process - recommend starting with 5,000 or less to avoid timeouts\n",
        "start_message = 1  # Starting message number for processing - adjust as needed to begin script from a specific message\n",
        "\n",
        "# Process the specified subset with a message limit - this will rank messages and save the results\n",
        "process_subset(subset_name, output_file, max_messages, start_message)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}