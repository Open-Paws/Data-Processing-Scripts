{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcPaeHGmPGcX"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Define your GitHub username and personal access token\n",
        "username = 'ADD_YOUR_USERNAME_HERE'\n",
        "token = 'ADD_YOUR_ACCESS_TOKEN_HERE\n",
        "\n",
        "# GitHub API URL to fetch starred repositories\n",
        "url = f'https://api.github.com/users/{username}/starred'\n",
        "\n",
        "# Headers for GitHub API request\n",
        "headers = {\n",
        "    'Authorization': f'token {token}',\n",
        "    'Accept': 'application/vnd.github.v3+json'\n",
        "}\n",
        "\n",
        "# Function to get the content of the repository\n",
        "def get_repo_content(repo_full_name):\n",
        "    print(f\"Fetching content for repository: {repo_full_name}\")\n",
        "    content_url = f'https://api.github.com/repos/{repo_full_name}/contents'\n",
        "    content_response = requests.get(content_url, headers=headers)\n",
        "\n",
        "    if content_response.status_code == 200:\n",
        "        return content_response.json()\n",
        "    else:\n",
        "        print(f\"Failed to get contents for {repo_full_name}, status code: {content_response.status_code}\")\n",
        "        return []\n",
        "\n",
        "# Function to split content into chunks, ensuring no split occurs mid-line\n",
        "def split_content(content, chunk_size=5000):\n",
        "    lines = content.splitlines(keepends=True)  # Preserve line breaks\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        if len(current_chunk) + len(line) <= chunk_size:\n",
        "            current_chunk += line\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = line\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to check if the file is text or binary (like PDF)\n",
        "def is_text_file(content_type, file_extension):\n",
        "    binary_file_extensions = ['.pdf', '.png', '.jpg', '.jpeg', '.gif', '.zip', '.exe', '.tar', '.gz', '.rar']\n",
        "    if any(file_extension.lower().endswith(ext) for ext in binary_file_extensions):\n",
        "        return False\n",
        "    if 'text' in content_type or 'json' in content_type:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Function to fetch starred repositories with pagination and save them to a CSV\n",
        "def fetch_starred_repos():\n",
        "    page = 1\n",
        "    has_more = True\n",
        "\n",
        "    print(\"Opening CSV file for writing...\")\n",
        "    with open('starred_repos.csv', mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "        writer.writerow(['Repo Name', 'Repo URL', 'File Path', 'Chunk Number', 'File Content'])\n",
        "\n",
        "        while has_more:\n",
        "            print(f\"Fetching page {page} of starred repositories...\")\n",
        "            response = requests.get(f'{url}?page={page}', headers=headers)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Failed to fetch starred repositories on page {page}: {response.status_code}\")\n",
        "                return\n",
        "\n",
        "            starred_repos = response.json()\n",
        "            print(f\"Processing {len(starred_repos)} repositories on page {page}...\")\n",
        "\n",
        "            if len(starred_repos) == 0:\n",
        "                print(f\"No more repositories to fetch on page {page}.\")\n",
        "                has_more = False\n",
        "                break\n",
        "\n",
        "            for repo in starred_repos:\n",
        "                repo_name = repo['name']\n",
        "                repo_url = repo['html_url']\n",
        "                repo_full_name = repo['full_name']\n",
        "\n",
        "                # Fetch repository content\n",
        "                repo_content = get_repo_content(repo_full_name)\n",
        "\n",
        "                for item in repo_content:\n",
        "                    # Only handle files, not directories\n",
        "                    if item['type'] == 'file':\n",
        "                        file_path = item['path']\n",
        "                        file_content_url = item['download_url']\n",
        "\n",
        "                        # Fetch the file content\n",
        "                        try:\n",
        "                            file_content_response = requests.get(file_content_url)\n",
        "                            file_content_type = file_content_response.headers.get('Content-Type', '')\n",
        "                            file_extension = file_path.split('.')[-1] if '.' in file_path else ''\n",
        "\n",
        "                            # Skip binary files like PDFs\n",
        "                            if not is_text_file(file_content_type, file_extension):\n",
        "                                print(f\"Skipping binary file: {file_path} (type: {file_content_type})\")\n",
        "                                continue\n",
        "\n",
        "                            file_content = file_content_response.text\n",
        "                        except Exception as e:\n",
        "                            print(f\"Failed to fetch content for {file_path}: {e}\")\n",
        "                            continue\n",
        "\n",
        "                        # Split the file content into chunks and write them to the CSV\n",
        "                        chunks = split_content(file_content, chunk_size=5000)\n",
        "\n",
        "                        for i, chunk in enumerate(chunks, start=1):\n",
        "                            writer.writerow([repo_name, repo_url, file_path, i, chunk])\n",
        "\n",
        "            page += 1\n",
        "            print(f\"Moving to next page: {page}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    print(\"CSV file written successfully!\")\n",
        "\n",
        "# Run the function to fetch starred repos and generate CSV\n",
        "fetch_starred_repos()\n",
        "\n",
        "from google.colab import files\n",
        "print(\"Downloading CSV file...\")\n",
        "files.download('starred_repos.csv')\n",
        "print(\"Download initiated.\")"
      ]
    }
  ]
}