{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sLpc3zOzycf1",
        "outputId": "baa5e57f-137c-485f-d809-b715748da1ef"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.parse\n",
        "from urllib3.util import Retry\n",
        "from requests.adapters import HTTPAdapter\n",
        "import csv\n",
        "import re\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "def clean_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    clean_path = re.sub(r'\\/{2,}', '/', parsed_url.path)  # Remove duplicate slashes\n",
        "    clean_url = urllib.parse.urlunparse(parsed_url._replace(path=clean_path, query='', fragment=''))\n",
        "    return clean_url\n",
        "\n",
        "def get_all_links(url, domain):\n",
        "    response = session.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    links = set()\n",
        "\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        link = urllib.parse.urljoin(url, a_tag['href'])\n",
        "        clean_link = clean_url(link)\n",
        "        if domain in urllib.parse.urlparse(clean_link).netloc:\n",
        "            links.add(clean_link)\n",
        "\n",
        "    return links\n",
        "\n",
        "def get_all_urls(base_url):\n",
        "    to_visit = set([base_url])\n",
        "    visited = set()\n",
        "    all_urls = set()\n",
        "    domain = urllib.parse.urlparse(base_url).netloc\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "\n",
        "        visited.add(current_url)\n",
        "        all_urls.add(current_url)\n",
        "        print(f\"Collecting links from: {current_url}\")\n",
        "\n",
        "        try:\n",
        "            # Get all links from the current page\n",
        "            links = get_all_links(current_url, domain)\n",
        "            to_visit.update(links - visited)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if e.response and e.response.status_code == 429:\n",
        "                print(f\"Rate limit exceeded. Waiting for 70 seconds before retrying...\") # Rate limiting - adjust as needed\n",
        "                time.sleep(70) # Wait for 70 seconds before retrying (rate limiting - adjust as needed)\n",
        "                to_visit.add(current_url)  # Retry the current URL\n",
        "                continue\n",
        "            print(f\"Error collecting links from {current_url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(15)  # Wait for 15 seconds between requests (rate limiting - adjust as needed)\n",
        "\n",
        "    return all_urls\n",
        "\n",
        "def extract_content(url):\n",
        "    response = session.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    content = []\n",
        "    position = 1\n",
        "\n",
        "    # Extract text and images based on tags\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'img']):\n",
        "        if tag.name == 'img':\n",
        "            img_url = tag.get('src')\n",
        "            if img_url:\n",
        "                img_url = urllib.parse.urljoin(url, img_url)\n",
        "                content.append((url, img_url, 'image', position))\n",
        "                position += 1\n",
        "        else:\n",
        "            text = tag.get_text(strip=True)\n",
        "            if text:\n",
        "                content.append((url, text, 'text', position))\n",
        "                position += 1\n",
        "\n",
        "    return content\n",
        "\n",
        "def scrape_all_content(urls):\n",
        "    all_content = []\n",
        "    for url in urls:\n",
        "        print(f\"Scraping content from: {url}\")\n",
        "\n",
        "        try:\n",
        "            # Extract content from the current page\n",
        "            content = extract_content(url)\n",
        "            all_content.extend(content)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            if e.response and e.response.status_code == 429:\n",
        "                print(f\"Rate limit exceeded. Waiting for 70 seconds before retrying...\") # Rate limiting - adjust as needed\n",
        "                time.sleep(70)\n",
        "                # Retry the current URL after delay (rate limiting - adjust as needed)\n",
        "                try:\n",
        "                    content = extract_content(url)\n",
        "                    all_content.extend(content)\n",
        "                except Exception as retry_e:\n",
        "                    print(f\"Error scraping {url} after retry: {retry_e}\")\n",
        "            else:\n",
        "                print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "        time.sleep(15)  # Wait for 15 seconds between requests (rate limiting - adjust as needed)\n",
        "\n",
        "    return all_content\n",
        "\n",
        "# Create a session with retry logic\n",
        "session = requests.Session()\n",
        "retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
        "session.mount('http://', HTTPAdapter(max_retries=retries))\n",
        "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
        "\n",
        "# SET WEBSITE TO SCRAPE HERE\n",
        "base_url = 'https://YOUR_URL_GOES_HERE.com'  # Replace with your target domain, e.g., 'https://example.com' (this is the starting point for the scraper)\n",
        "all_urls = get_all_urls(base_url)\n",
        "all_content = scrape_all_content(all_urls)\n",
        "\n",
        "# Export to CSV\n",
        "csv_filename = 'EXPORT_FILE_NAME.csv' # Replace with the desired filename for the CSV output\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['url', 'content', 'type', 'position'])  # Header row\n",
        "    for row in all_content:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Website content has been saved to {csv_filename}\")\n",
        "\n",
        "# Download the CSV file to the local device\n",
        "files.download(csv_filename)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
