{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofbrdhHNFJ7H"
      },
      "outputs": [],
      "source": [
        "# Install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install selenium requests beautifulsoup4 webdriver-manager google.colab\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "import urllib.parse\n",
        "from requests.exceptions import RequestException, Timeout, ConnectionError, HTTPError\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager  # Use webdriver-manager to install the correct version of ChromeDriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Your 2Captcha API key\n",
        "API_KEY = \"68d2820adc18daf84182efc962c68acd\"  # Replace with your actual 2Captcha API key\n",
        "\n",
        "# Initialize Selenium WebDriver (headless Chrome) with ChromeDriverManager\n",
        "def init_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # Automatically download and set up ChromeDriver with ChromeDriverManager\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to solve CAPTCHA using 2Captcha\n",
        "def solve_captcha(driver, site_key, url):\n",
        "    if not site_key:\n",
        "        print(\"No site key found, skipping CAPTCHA solving.\")\n",
        "        return None\n",
        "\n",
        "    # Send CAPTCHA request to 2Captcha\n",
        "    captcha_id_response = requests.post(\n",
        "        \"http://2captcha.com/in.php\",\n",
        "        data={\n",
        "            'key': API_KEY,\n",
        "            'method': 'userrecaptcha',\n",
        "            'googlekey': site_key,\n",
        "            'pageurl': url,\n",
        "            'json': 1\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    if captcha_id_response[\"status\"] == 1:\n",
        "        captcha_id = captcha_id_response[\"request\"]\n",
        "        print(\"CAPTCHA sent for solving...\")\n",
        "        # Poll for solution\n",
        "        fetch_url = f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "        while True:\n",
        "            time.sleep(5)  # Delay before each poll\n",
        "            response = requests.get(fetch_url).json()\n",
        "            if response[\"status\"] == 1:\n",
        "                print(\"CAPTCHA solved\")\n",
        "                return response[\"request\"]\n",
        "            elif response[\"status\"] == 0 and response[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "                print(\"Waiting for CAPTCHA to be solved...\")\n",
        "            else:\n",
        "                print(\"Error solving CAPTCHA:\", response)\n",
        "                return None\n",
        "    else:\n",
        "        print(\"Error submitting CAPTCHA:\", captcha_id_response)\n",
        "        return None\n",
        "\n",
        "# Function to extract the site_key automatically\n",
        "def get_site_key(driver):\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    # Look for 'data-sitekey' attributes in the page\n",
        "    site_key = None\n",
        "    recaptcha_div = soup.find('div', {'class': 'g-recaptcha', 'data-sitekey': True})\n",
        "    if recaptcha_div:\n",
        "        site_key = recaptcha_div['data-sitekey']\n",
        "    else:\n",
        "        # Try to find the key in a script tag if not in div\n",
        "        script_tags = soup.find_all('script')\n",
        "        for script in script_tags:\n",
        "            if 'data-sitekey' in str(script):\n",
        "                match = re.search(r'data-sitekey=\"(.+?)\"', str(script))\n",
        "                if match:\n",
        "                    site_key = match.group(1)\n",
        "                    break\n",
        "\n",
        "    if site_key:\n",
        "        print(f\"Found site key: {site_key}\")\n",
        "    else:\n",
        "        print(\"Site key not found\")\n",
        "    return site_key\n",
        "\n",
        "# Function to clean URLs\n",
        "def clean_url(url):\n",
        "  parsed_url = urllib.parse.urlparse(url)\n",
        "  path_segments = parsed_url.path.split('/')\n",
        "  cleaned_segments = []\n",
        "  for i, segment in enumerate(path_segments):\n",
        "    if segment and (i < 2 or segment != path_segments[i - 2]):  # Check if segment is not empty and not repeating the pattern\n",
        "      cleaned_segments.append(segment)\n",
        "  cleaned_path = '/'.join(cleaned_segments)\n",
        "  clean_url = urllib.parse.urlunparse(parsed_url._replace(path=cleaned_path, query='', fragment=''))\n",
        "  return clean_url\n",
        "\n",
        "# Text chunking function\n",
        "def split_text_into_chunks(text, max_chunk_size=5000):\n",
        "    # Split the text into sentences using regex that detects sentence endings.\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Join current chunk and check its size.\n",
        "        if len(' '.join(current_chunk + [sentence])) <= max_chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "        else:\n",
        "            # If adding this sentence exceeds the chunk size, start a new chunk.\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "\n",
        "    # Add the last chunk (if any).\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to extract content from a webpage\n",
        "def extract_content(driver, url):\n",
        "  try:\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Automatically find the site_key for CAPTCHA\n",
        "    site_key = get_site_key(driver)\n",
        "\n",
        "    # If CAPTCHA detected, solve it\n",
        "    if site_key:\n",
        "        captcha_solution = solve_captcha(driver, site_key, url)\n",
        "        if captcha_solution:\n",
        "            driver.execute_script(f'document.getElementById(\"g-recaptcha-response\").innerHTML=\"{captcha_solution}\";')\n",
        "            driver.execute_script('document.querySelector(\"form\").submit();')\n",
        "            time.sleep(5)\n",
        "            driver.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    content = []\n",
        "    position = 1\n",
        "\n",
        "    exclude_patterns = [\n",
        "        re.compile(r'header', re.IGNORECASE),\n",
        "        re.compile(r'footer', re.IGNORECASE),\n",
        "        re.compile(r'navigation', re.IGNORECASE),\n",
        "        re.compile(r'nav', re.IGNORECASE),\n",
        "        re.compile(r'ads', re.IGNORECASE),\n",
        "        re.compile(r'sidebar', re.IGNORECASE),\n",
        "        re.compile(r'banner', re.IGNORECASE),\n",
        "        re.compile(r'popup', re.IGNORECASE),\n",
        "        re.compile(r'button', re.IGNORECASE),\n",
        "        re.compile(r'script', re.IGNORECASE),\n",
        "        re.compile(r'style', re.IGNORECASE),\n",
        "        re.compile(r'widget', re.IGNORECASE),\n",
        "        re.compile(r'social', re.IGNORECASE),\n",
        "        re.compile(r'comment', re.IGNORECASE),\n",
        "        re.compile(r'search', re.IGNORECASE),\n",
        "        re.compile(r'breadcrumb', re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    # Collect all text content first\n",
        "    current_text = ''\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p']):\n",
        "        # Skip excluded elements\n",
        "        if any(pattern.search(tag.get('class', [''])[0]) for pattern in exclude_patterns if tag.get('class')) or \\\n",
        "           any(pattern.search(tag.get('id', '')) for pattern in exclude_patterns) or \\\n",
        "           any(pattern.search(tag.name) for pattern in exclude_patterns):\n",
        "            continue\n",
        "\n",
        "        text = tag.get_text(strip=True)\n",
        "        if text:\n",
        "            current_text += ' ' + text  # Accumulate text\n",
        "\n",
        "    # Split accumulated text into chunks\n",
        "    chunks = split_text_into_chunks(current_text.strip())\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        content.append((url, chunk, 'text', i + 1))\n",
        "\n",
        "    # Now process images separately\n",
        "    position = len(chunks) + 1  # Start position after text chunks\n",
        "    for tag in soup.find_all('img'):\n",
        "        # Skip excluded elements\n",
        "        if any(pattern.search(tag.get('class', [''])[0]) for pattern in exclude_patterns if tag.get('class')) or \\\n",
        "           any(pattern.search(tag.get('id', '')) for pattern in exclude_patterns) or \\\n",
        "           any(pattern.search(tag.name) for pattern in exclude_patterns):\n",
        "            continue\n",
        "\n",
        "        img_url = tag.get('src')\n",
        "        if img_url:\n",
        "            img_url = urllib.parse.urljoin(url, img_url)\n",
        "            content.append((url, img_url, 'image', position))\n",
        "            position += 1\n",
        "\n",
        "    # After the loop, process any remaining text\n",
        "    if current_text.strip():\n",
        "        chunks = split_text_into_chunks(current_text.strip())\n",
        "        for chunk in chunks:\n",
        "            content.append((url, chunk, 'text', position))\n",
        "            position += 1\n",
        "\n",
        "    return content\n",
        "\n",
        "  except HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "        if http_err.response.status_code == 404:\n",
        "            print(f\"Page not found: {url}\")\n",
        "        return []\n",
        "\n",
        "  except Timeout as timeout_err:\n",
        "        print(f\"Timeout error occurred: {timeout_err}\")\n",
        "        return []\n",
        "\n",
        "  except ConnectionError as connection_err:\n",
        "        print(f\"Connection error occurred: {connection_err}\")\n",
        "        return []\n",
        "\n",
        "  except RequestException as e:\n",
        "        # Check for specific error codes here, if hasattr(e, 'response') and 'status_code' in e.response:\n",
        "        if hasattr(e, 'response') and e.response.status_code in [403, 500, 502, 503, 504]:\n",
        "            print(f\"Error accessing {url}: Status Code {e.response.status_code}\")\n",
        "            return []\n",
        "\n",
        "        # Handle other unexpected errors\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to get all internal links from a webpage\n",
        "def get_all_links(driver, url, domain):\n",
        "    links = set()\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            href = urllib.parse.urljoin(url, href)\n",
        "            parsed_href = urllib.parse.urlparse(href)\n",
        "            if parsed_href.netloc == domain:\n",
        "                clean_href = clean_url(href)\n",
        "                links.add(clean_href)\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting links from {url}: {e}\")\n",
        "    return links\n",
        "\n",
        "# Function to crawl the website and collect all URLs\n",
        "def crawl_website(driver, base_url):\n",
        "    domain = urllib.parse.urlparse(base_url).netloc\n",
        "    to_visit = set([base_url])\n",
        "    visited = set()\n",
        "    all_urls = []\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "\n",
        "        # Clean the URL before checking if it's visited\n",
        "        clean_current_url = clean_url(current_url)\n",
        "\n",
        "        if clean_current_url in visited:\n",
        "            continue\n",
        "\n",
        "        print(f\"Crawling: {current_url}\")\n",
        "        visited.add(clean_current_url)  # Add the cleaned URL to visited set\n",
        "        all_urls.append(clean_current_url)  # Add the cleaned URL to the list\n",
        "\n",
        "        # Get all internal links from the current page\n",
        "        links = get_all_links(driver, current_url, domain)\n",
        "\n",
        "        # Clean the links before adding them to to_visit\n",
        "        clean_links = set(clean_url(link) for link in links)\n",
        "        to_visit.update(clean_links - visited)\n",
        "\n",
        "    return all_urls\n",
        "\n",
        "# Function to scrape all content from a list of URLs\n",
        "def scrape_all_content(driver, urls):\n",
        "    all_content = []\n",
        "    for url in urls:\n",
        "        url = clean_url(url)\n",
        "        print(f\"Scraping content from: {url}\")\n",
        "\n",
        "        try:\n",
        "            content = extract_content(driver, url)\n",
        "            all_content.extend(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(2)  # Wait for 2 seconds between requests\n",
        "\n",
        "    return all_content\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL of the website to crawl\n",
        "    base_url = \"INSERT_URL_HERE/\"  # Replace with your target website\n",
        "\n",
        "    # Initialize the driver\n",
        "    driver = init_driver()\n",
        "\n",
        "    # Crawl the website to get all URLs\n",
        "    all_urls = crawl_website(driver, base_url)\n",
        "    print(f\"Total URLs collected: {len(all_urls)}\")\n",
        "\n",
        "    # Scrape content from the collected URLs\n",
        "    all_content = scrape_all_content(driver, all_urls)\n",
        "\n",
        "    # Export to CSV\n",
        "    csv_filename = 'NAME_FILE_HERE'\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
        "        writer.writerow(['url', 'content', 'type', 'position'])  # Header row\n",
        "        for row in all_content:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Website content has been saved to {csv_filename}\")\n",
        "\n",
        "    # Download the CSV file to the local device\n",
        "    files.download(csv_filename)\n",
        "\n",
        "    # Clean up and close the driver\n",
        "    driver.quit()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}