{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5rMarctLmy7"
      },
      "outputs": [],
      "source": [
        "# Install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install selenium requests beautifulsoup4 webdriver-manager google.colab\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "import urllib.parse\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager  # Use webdriver-manager to install the correct version of ChromeDriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "\n",
        "# Your 2Captcha API key\n",
        "API_KEY = \"YOUR_API_KEY\"  # Replace with your actual 2Captcha API key\n",
        "\n",
        "# Initialize Selenium WebDriver (headless Chrome) with ChromeDriverManager\n",
        "def init_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # Automatically download and set up ChromeDriver with ChromeDriverManager\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to solve CAPTCHA using 2Captcha\n",
        "def solve_captcha(driver, site_key, url):\n",
        "    if not site_key:\n",
        "        print(\"No site key found, skipping CAPTCHA solving.\")\n",
        "        return None\n",
        "\n",
        "    # Send CAPTCHA request to 2Captcha\n",
        "    captcha_id_response = requests.post(\n",
        "        \"http://2captcha.com/in.php\",\n",
        "        data={\n",
        "            'key': API_KEY,\n",
        "            'method': 'userrecaptcha',\n",
        "            'googlekey': site_key,\n",
        "            'pageurl': url,\n",
        "            'json': 1\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    if captcha_id_response[\"status\"] == 1:\n",
        "        captcha_id = captcha_id_response[\"request\"]\n",
        "        print(\"CAPTCHA sent for solving...\")\n",
        "        # Poll for solution\n",
        "        fetch_url = f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "        while True:\n",
        "            time.sleep(5)  # Delay before each poll\n",
        "            response = requests.get(fetch_url).json()\n",
        "            if response[\"status\"] == 1:\n",
        "                print(\"CAPTCHA solved\")\n",
        "                return response[\"request\"]\n",
        "            elif response[\"status\"] == 0 and response[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "                print(\"Waiting for CAPTCHA to be solved...\")\n",
        "            else:\n",
        "                print(\"Error solving CAPTCHA:\", response)\n",
        "                return None\n",
        "    else:\n",
        "        print(\"Error submitting CAPTCHA:\", captcha_id_response)\n",
        "        return None\n",
        "\n",
        "# Function to extract the site_key automatically\n",
        "def get_site_key(driver):\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    # Look for 'data-sitekey' attributes in the page\n",
        "    site_key = None\n",
        "    recaptcha_div = soup.find('div', {'class': 'g-recaptcha', 'data-sitekey': True})\n",
        "    if recaptcha_div:\n",
        "        site_key = recaptcha_div['data-sitekey']\n",
        "    else:\n",
        "        # Try to find the key in a script tag if not in div\n",
        "        script_tags = soup.find_all('script')\n",
        "        for script in script_tags:\n",
        "            if 'data-sitekey' in str(script):\n",
        "                match = re.search(r'data-sitekey=\"(.+?)\"', str(script))\n",
        "                if match:\n",
        "                    site_key = match.group(1)\n",
        "                    break\n",
        "\n",
        "    if site_key:\n",
        "        print(f\"Found site key: {site_key}\")\n",
        "    else:\n",
        "        print(\"Site key not found\")\n",
        "    return site_key\n",
        "\n",
        "# Function to clean URLs\n",
        "def clean_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    clean_path = re.sub(r'/+', '/', parsed_url.path)  # Remove duplicate slashes\n",
        "    clean_url = urllib.parse.urlunparse(parsed_url._replace(path=clean_path, query='', fragment=''))\n",
        "    return clean_url\n",
        "\n",
        "# Text chunking function\n",
        "def split_text_into_chunks(text, max_chunk_size=2000):\n",
        "    # Split the text into sentences using regex that detects sentence endings.\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Join current chunk and check its size.\n",
        "        if len(' '.join(current_chunk + [sentence])) <= max_chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "        else:\n",
        "            # If adding this sentence exceeds the chunk size, start a new chunk.\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "\n",
        "    # Add the last chunk (if any).\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Function to extract content from a webpage\n",
        "def extract_content(driver, url):\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Automatically find the site_key for CAPTCHA\n",
        "    site_key = get_site_key(driver)\n",
        "\n",
        "    # If CAPTCHA detected, solve it\n",
        "    if site_key:\n",
        "        captcha_solution = solve_captcha(driver, site_key, url)\n",
        "        if captcha_solution:\n",
        "            driver.execute_script(f'document.getElementById(\"g-recaptcha-response\").innerHTML=\"{captcha_solution}\";')\n",
        "            driver.execute_script('document.querySelector(\"form\").submit();')\n",
        "            time.sleep(5)\n",
        "            driver.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    content = []\n",
        "    position = 1\n",
        "\n",
        "    exclude_patterns = [\n",
        "        re.compile(r'header', re.IGNORECASE),\n",
        "        re.compile(r'footer', re.IGNORECASE),\n",
        "        re.compile(r'navigation', re.IGNORECASE),\n",
        "        re.compile(r'nav', re.IGNORECASE),\n",
        "        re.compile(r'ads', re.IGNORECASE),\n",
        "        re.compile(r'sidebar', re.IGNORECASE),\n",
        "        re.compile(r'banner', re.IGNORECASE),\n",
        "        re.compile(r'popup', re.IGNORECASE),\n",
        "        re.compile(r'button', re.IGNORECASE),\n",
        "        re.compile(r'script', re.IGNORECASE),\n",
        "        re.compile(r'style', re.IGNORECASE),\n",
        "        re.compile(r'widget', re.IGNORECASE),\n",
        "        re.compile(r'social', re.IGNORECASE),\n",
        "        re.compile(r'comment', re.IGNORECASE),\n",
        "        re.compile(r'search', re.IGNORECASE),\n",
        "        re.compile(r'breadcrumb', re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    current_text = ''\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'img']):\n",
        "        # Skip excluded elements\n",
        "        if any(pattern.search(tag.get('class', [''])[0]) for pattern in exclude_patterns if tag.get('class')) or \\\n",
        "           any(pattern.search(tag.get('id', '')) for pattern in exclude_patterns) or \\\n",
        "           any(pattern.search(tag.name) for pattern in exclude_patterns):\n",
        "            continue\n",
        "\n",
        "        if tag.name == 'img':\n",
        "            # Before adding the image, process any accumulated text\n",
        "            if current_text.strip():\n",
        "                # Split accumulated text into chunks\n",
        "                chunks = split_text_into_chunks(current_text.strip())\n",
        "                for chunk in chunks:\n",
        "                    content.append((url, chunk, 'text', position))\n",
        "                    position += 1\n",
        "                current_text = ''\n",
        "            # Process the image\n",
        "            img_url = tag.get('src')\n",
        "            if img_url:\n",
        "                img_url = urllib.parse.urljoin(url, img_url)\n",
        "                content.append((url, img_url, 'image', position))\n",
        "                position += 1\n",
        "        else:\n",
        "            text = tag.get_text(strip=True)\n",
        "            if text:\n",
        "                current_text += ' ' + text  # Accumulate text\n",
        "                # Check if current_text exceeds max_chunk_size\n",
        "                if len(current_text) >= 2000:\n",
        "                    chunks = split_text_into_chunks(current_text.strip())\n",
        "                    for chunk in chunks:\n",
        "                        content.append((url, chunk, 'text', position))\n",
        "                        position += 1\n",
        "                    current_text = ''  # Reset after processing\n",
        "\n",
        "    # After the loop, process any remaining text\n",
        "    if current_text.strip():\n",
        "        chunks = split_text_into_chunks(current_text.strip())\n",
        "        for chunk in chunks:\n",
        "            content.append((url, chunk, 'text', position))\n",
        "            position += 1\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to get all internal links from a webpage\n",
        "def get_all_links(driver, url, domain):\n",
        "    links = set()\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        time.sleep(2)\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            href = urllib.parse.urljoin(url, href)\n",
        "            parsed_href = urllib.parse.urlparse(href)\n",
        "            if parsed_href.netloc == domain:\n",
        "                clean_href = clean_url(href)\n",
        "                links.add(clean_href)\n",
        "    except Exception as e:\n",
        "        print(f\"Error collecting links from {url}: {e}\")\n",
        "    return links\n",
        "\n",
        "# Function to crawl the website and collect all URLs\n",
        "def crawl_website(driver, base_url):\n",
        "    domain = urllib.parse.urlparse(base_url).netloc\n",
        "    to_visit = set([base_url])\n",
        "    visited = set()\n",
        "    all_urls = []\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop()\n",
        "        if current_url in visited:\n",
        "            continue\n",
        "        print(f\"Crawling: {current_url}\")\n",
        "        visited.add(current_url)\n",
        "        all_urls.append(current_url)\n",
        "\n",
        "        # Get all internal links from the current page\n",
        "        links = get_all_links(driver, current_url, domain)\n",
        "        to_visit.update(links - visited)\n",
        "\n",
        "    return all_urls\n",
        "\n",
        "# Function to scrape all content from a list of URLs\n",
        "def scrape_all_content(driver, urls):\n",
        "    all_content = []\n",
        "    for url in urls:\n",
        "        url = clean_url(url)\n",
        "        print(f\"Scraping content from: {url}\")\n",
        "\n",
        "        try:\n",
        "            content = extract_content(driver, url)\n",
        "            all_content.extend(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(2)  # Wait for 2 seconds between requests\n",
        "\n",
        "    return all_content\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Base URL of the website to crawl\n",
        "    base_url = \"https://TARGETURL.com\"  # Replace with your target website\n",
        "\n",
        "    # Initialize the driver\n",
        "    driver = init_driver()\n",
        "\n",
        "    # Crawl the website to get all URLs\n",
        "    all_urls = crawl_website(driver, base_url)\n",
        "    print(f\"Total URLs collected: {len(all_urls)}\")\n",
        "\n",
        "    # Scrape content from the collected URLs\n",
        "    all_content = scrape_all_content(driver, all_urls)\n",
        "\n",
        "    # Export to CSV\n",
        "    csv_filename = 'EXPORT_FILE_NAME.csv' # Replace with the actual name you want to export the file as\n",
        "    with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "        writer = csv.writer(file, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
        "        writer.writerow(['url', 'content', 'type', 'position'])  # Header row\n",
        "        for row in all_content:\n",
        "            writer.writerow(row)\n",
        "\n",
        "    print(f\"Website content has been saved to {csv_filename}\")\n",
        "\n",
        "    # Download the CSV file to the local device\n",
        "    files.download(csv_filename)\n",
        "\n",
        "    # Clean up and close the driver\n",
        "    driver.quit()"
      ]
    }
  ]
}