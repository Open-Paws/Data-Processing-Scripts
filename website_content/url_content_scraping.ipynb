{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install Google Chrome\n",
        "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "!dpkg -i google-chrome-stable_current_amd64.deb\n",
        "!apt-get -f install -y\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install selenium requests beautifulsoup4 webdriver-manager google.colab\n",
        "\n",
        "import time\n",
        "import csv\n",
        "import re\n",
        "import requests\n",
        "import urllib.parse\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from webdriver_manager.chrome import ChromeDriverManager  # Use webdriver-manager to install the correct version of ChromeDriver\n",
        "\n",
        "# Your 2Captcha API key\n",
        "API_KEY = \"YOUR_KEY_GOES_HERE\"  # Replace with your actual 2Captcha API key\n",
        "\n",
        "# Initialize Selenium WebDriver (headless Chrome) with ChromeDriverManager\n",
        "def init_driver():\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--disable-gpu\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # Automatically download and set up ChromeDriver with ChromeDriverManager\n",
        "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
        "    return driver\n",
        "\n",
        "# Function to solve CAPTCHA using 2Captcha\n",
        "def solve_captcha(driver, site_key, url):\n",
        "    if not site_key:\n",
        "        print(\"No site key found, skipping CAPTCHA solving.\")\n",
        "        return None\n",
        "\n",
        "    # Send CAPTCHA request to 2Captcha\n",
        "    captcha_id_response = requests.post(\n",
        "        \"http://2captcha.com/in.php\",\n",
        "        data={\n",
        "            'key': API_KEY,\n",
        "            'method': 'userrecaptcha',\n",
        "            'googlekey': site_key,\n",
        "            'pageurl': url,\n",
        "            'json': 1\n",
        "        }\n",
        "    ).json()\n",
        "\n",
        "    if captcha_id_response[\"status\"] == 1:\n",
        "        captcha_id = captcha_id_response[\"request\"]\n",
        "        print(\"CAPTCHA sent for solving...\")\n",
        "        # Poll for solution\n",
        "        fetch_url = f\"http://2captcha.com/res.php?key={API_KEY}&action=get&id={captcha_id}&json=1\"\n",
        "        while True:\n",
        "            time.sleep(5)  # Delay before each poll\n",
        "            response = requests.get(fetch_url).json()\n",
        "            if response[\"status\"] == 1:\n",
        "                print(\"CAPTCHA solved\")\n",
        "                return response[\"request\"]\n",
        "            elif response[\"status\"] == 0 and response[\"request\"] == \"CAPCHA_NOT_READY\":\n",
        "                print(\"Waiting for CAPTCHA to be solved...\")\n",
        "            else:\n",
        "                print(\"Error solving CAPTCHA:\", response)\n",
        "                return None\n",
        "    else:\n",
        "        print(\"Error submitting CAPTCHA:\", captcha_id_response)\n",
        "        return None\n",
        "\n",
        "# Function to extract the site_key automatically\n",
        "def get_site_key(driver):\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    # Look for 'data-sitekey' attributes in the page\n",
        "    site_key = None\n",
        "    recaptcha_div = soup.find('div', {'class': 'g-recaptcha', 'data-sitekey': True})\n",
        "    if recaptcha_div:\n",
        "        site_key = recaptcha_div['data-sitekey']\n",
        "    else:\n",
        "        # Try to find the key in a script tag if not in div\n",
        "        script_tags = soup.find_all('script')\n",
        "        for script in script_tags:\n",
        "            if 'data-sitekey' in str(script):\n",
        "                match = re.search(r'data-sitekey=\"(.+?)\"', str(script))\n",
        "                if match:\n",
        "                    site_key = match.group(1)\n",
        "                    break\n",
        "\n",
        "    if site_key:\n",
        "        print(f\"Found site key: {site_key}\")\n",
        "    else:\n",
        "        print(\"Site key not found\")\n",
        "    return site_key\n",
        "\n",
        "# Function to clean URLs\n",
        "def clean_url(url):\n",
        "    parsed_url = urllib.parse.urlparse(url)\n",
        "    clean_path = re.sub(r'/{2,}', '/', parsed_url.path)  # Remove duplicate slashes\n",
        "    clean_url = urllib.parse.urlunparse(parsed_url._replace(path=clean_path, query='', fragment=''))\n",
        "    return clean_url\n",
        "\n",
        "# Text chunking function\n",
        "def split_text_into_chunks(text, max_chunk_size=2000):\n",
        "    # Split the text into sentences using regex that detects sentence endings.\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text.strip())\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        # Join current chunk and check its size.\n",
        "        if len(' '.join(current_chunk + [sentence])) <= max_chunk_size:\n",
        "            current_chunk.append(sentence)\n",
        "        else:\n",
        "            # If adding this sentence exceeds the chunk size, start a new chunk.\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "\n",
        "    # Add the last chunk (if any).\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "# Updated extract_content function\n",
        "def extract_content(driver, url):\n",
        "    driver.get(url)\n",
        "\n",
        "    # Wait for the page to load\n",
        "    time.sleep(2)\n",
        "\n",
        "    # Automatically find the site_key for CAPTCHA\n",
        "    site_key = get_site_key(driver)\n",
        "\n",
        "    # If CAPTCHA detected, solve it\n",
        "    if site_key:\n",
        "        captcha_solution = solve_captcha(driver, site_key, url)\n",
        "        if captcha_solution:\n",
        "            driver.execute_script(f'document.getElementById(\"g-recaptcha-response\").innerHTML=\"{captcha_solution}\";')\n",
        "            driver.execute_script('document.querySelector(\"form\").submit();')\n",
        "            time.sleep(5)\n",
        "            driver.get(url)\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "    content = []\n",
        "    position = 1\n",
        "\n",
        "    exclude_patterns = [\n",
        "        re.compile(r'header', re.IGNORECASE),\n",
        "        re.compile(r'footer', re.IGNORECASE),\n",
        "        re.compile(r'navigation', re.IGNORECASE),\n",
        "        re.compile(r'nav', re.IGNORECASE),\n",
        "        re.compile(r'ads', re.IGNORECASE),\n",
        "        re.compile(r'sidebar', re.IGNORECASE),\n",
        "        re.compile(r'banner', re.IGNORECASE),\n",
        "        re.compile(r'popup', re.IGNORECASE),\n",
        "        re.compile(r'button', re.IGNORECASE),\n",
        "        re.compile(r'script', re.IGNORECASE),\n",
        "        re.compile(r'style', re.IGNORECASE),\n",
        "        re.compile(r'widget', re.IGNORECASE),\n",
        "        re.compile(r'social', re.IGNORECASE),\n",
        "        re.compile(r'comment', re.IGNORECASE),\n",
        "        re.compile(r'search', re.IGNORECASE),\n",
        "        re.compile(r'breadcrumb', re.IGNORECASE),\n",
        "    ]\n",
        "\n",
        "    current_text = ''\n",
        "    for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'img']):\n",
        "        # Skip excluded elements\n",
        "        if any(pattern.search(tag.get('class', [''])[0]) for pattern in exclude_patterns if tag.get('class')) or \\\n",
        "           any(pattern.search(tag.get('id', '')) for pattern in exclude_patterns) or \\\n",
        "           any(pattern.search(tag.name) for pattern in exclude_patterns):\n",
        "            continue\n",
        "\n",
        "        if tag.name == 'img':\n",
        "            # Before adding the image, process any accumulated text\n",
        "            if current_text.strip():\n",
        "                # Split accumulated text into chunks\n",
        "                chunks = split_text_into_chunks(current_text.strip())\n",
        "                for chunk in chunks:\n",
        "                    content.append((url, chunk, 'text', position))\n",
        "                    position += 1\n",
        "                current_text = ''\n",
        "            # Process the image\n",
        "            img_url = tag.get('src')\n",
        "            if img_url:\n",
        "                img_url = urllib.parse.urljoin(url, img_url)\n",
        "                content.append((url, img_url, 'image', position))\n",
        "                position += 1\n",
        "        else:\n",
        "            text = tag.get_text(strip=True)\n",
        "            if text:\n",
        "                current_text += ' ' + text  # Accumulate text\n",
        "                # Check if current_text exceeds max_chunk_size\n",
        "                if len(current_text) >= 2000:\n",
        "                    chunks = split_text_into_chunks(current_text.strip())\n",
        "                    for chunk in chunks:\n",
        "                        content.append((url, chunk, 'text', position))\n",
        "                        position += 1\n",
        "                    current_text = ''  # Reset after processing\n",
        "\n",
        "    # After the loop, process any remaining text\n",
        "    if current_text.strip():\n",
        "        chunks = split_text_into_chunks(current_text.strip())\n",
        "        for chunk in chunks:\n",
        "            content.append((url, chunk, 'text', position))\n",
        "            position += 1\n",
        "\n",
        "    return content\n",
        "\n",
        "# Function to scrape all content from a list of URLs\n",
        "def scrape_all_content(driver, urls):\n",
        "    all_content = []\n",
        "    for url in urls:\n",
        "        url = clean_url(url)\n",
        "        print(f\"Scraping content from: {url}\")\n",
        "\n",
        "        try:\n",
        "            content = extract_content(driver, url)\n",
        "            all_content.extend(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "            continue\n",
        "\n",
        "        time.sleep(2)  # Wait for 2 seconds between requests\n",
        "\n",
        "    return all_content\n",
        "\n",
        "# Function to format multiline string of URLs\n",
        "def format_urls(multiline_string):\n",
        "    # Split the input string by new lines and strip extra spaces\n",
        "    url_list = [url.strip() for url in multiline_string.splitlines() if url.strip()]\n",
        "    return url_list\n",
        "\n",
        "# Multiline list of URLs provided\n",
        "multiline_urls = \"\"\"\n",
        "https://URL1.com\n",
        "https://URL2.com\n",
        "https://URL3.com\n",
        "\"\"\"  # Add more URLs as needed\n",
        "\n",
        "# Convert the multiline string to a list of URLs\n",
        "urls = format_urls(multiline_urls)\n",
        "\n",
        "# Initialize the driver and start scraping\n",
        "driver = init_driver()\n",
        "\n",
        "# Scrape content from the provided URLs\n",
        "all_content = scrape_all_content(driver, urls)\n",
        "\n",
        "# Export to CSV\n",
        "csv_filename = 'EXPORT_FILE_NAME.csv' # Replace with the actual name you want to export the file as\n",
        "with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file, escapechar='\\\\', quoting=csv.QUOTE_NONNUMERIC)\n",
        "    writer.writerow(['url', 'content', 'type', 'position'])  # Header row\n",
        "    for row in all_content:\n",
        "        writer.writerow(row)\n",
        "\n",
        "print(f\"Website content has been saved to {csv_filename}\")\n",
        "\n",
        "# Download the CSV file to the local device\n",
        "files.download(csv_filename)\n",
        "\n",
        "# Clean up and close the driver\n",
        "driver.quit()"
      ],
      "metadata": {
        "id": "Mrs0ZTelWESW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}