{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Perform bulk search for papers with strictly defined keywords"
      ],
      "metadata": {
        "id": "J-On1FFv5A1L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBirUTUf4-S6"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted successfully.\")\n",
        "\n",
        "# Set your API key here\n",
        "API_KEY = 'YOUR_API_KEY'\n",
        "print(\"API key set.\")\n",
        "\n",
        "# Define the highly refined and relevant query parameters\n",
        "queries = [\n",
        "    '\"veganism\"',\n",
        "    '\"animal rights\"',\n",
        "    \"speciesism\",\n",
        "    '\"vegan\"',\n",
        "    '\"vegetarian\"',\n",
        "    '\"vegetarianism\"',\n",
        "    '\"animal advocacy\"',\n",
        "    '\"animal ethics\"',\n",
        "    '\"animal liberation\"',\n",
        "    '\"plant-based diet\"'\n",
        "]\n",
        "\n",
        "print(\"Query parameters defined.\")\n",
        "\n",
        "fields = 'paperId,corpusId,url,title,venue,publicationVenue,year,authors,externalIds,abstract,referenceCount,citationCount,influentialCitationCount,isOpenAccess,openAccessPdf,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,journal,citationStyles'\n",
        "url = 'https://api.semanticscholar.org/graph/v1/paper/search/bulk'\n",
        "headers = {'x-api-key': API_KEY}\n",
        "delay = 1  # Delay in seconds\n",
        "\n",
        "print(\"Fields, URL, headers, and delay set.\")\n",
        "\n",
        "# Initialize an empty list to store paper details\n",
        "papers_list = []\n",
        "print(\"Initialized empty list for storing paper details.\")\n",
        "\n",
        "# Tracker file to save the progress\n",
        "tracker_file = '/content/drive/My Drive/bulk_progress_tracker.json'\n",
        "print(f\"Tracker file path set to: {tracker_file}\")\n",
        "\n",
        "# Load progress tracker if it exists\n",
        "if os.path.exists(tracker_file):\n",
        "    with open(tracker_file, 'r') as f:\n",
        "        progress_tracker = json.load(f)\n",
        "    print(\"Progress tracker loaded from file.\")\n",
        "else:\n",
        "    progress_tracker = {'query_index': 0, 'token': None}\n",
        "    print(\"No existing progress tracker found. Starting from the beginning.\")\n",
        "\n",
        "# Function to fetch papers for a specific query\n",
        "def fetch_papers(query, token=None):\n",
        "    print(f\"Fetching papers for query '{query}' with token {token}.\")\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'fields': fields,\n",
        "        'token': token\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        print(f\"Request sent. Status code: {response.status_code}\")\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"Other error occurred: {err}\")\n",
        "\n",
        "# Loop through each query and fetch papers\n",
        "query_index = progress_tracker['query_index']\n",
        "token = progress_tracker['token']\n",
        "print(f\"Starting loop with query_index: {query_index} and token: {token}\")\n",
        "\n",
        "while query_index < len(queries):\n",
        "    query = queries[query_index]\n",
        "    print(f\"Processing query {query_index + 1}/{len(queries)}: '{query}'\")\n",
        "    data = fetch_papers(query, token=token)\n",
        "    if data is None:\n",
        "        print(f\"Skipping query '{query}' due to error.\")\n",
        "        query_index += 1\n",
        "        token = None\n",
        "        continue\n",
        "    papers = data.get('data', [])\n",
        "    print(f\"Fetched {len(papers)} papers.\")\n",
        "    if not papers and not token:\n",
        "        print(f\"No papers found for query '{query}' with token {token}. Moving to next query.\")\n",
        "        query_index += 1\n",
        "        token = None\n",
        "        continue\n",
        "    papers_list.extend(papers)\n",
        "    print(f\"Added {len(papers)} papers to the list.\")\n",
        "    token = data.get('token')\n",
        "    if not token:\n",
        "        print(f\"No more papers to fetch for query '{query}'. Moving to next query.\")\n",
        "        query_index += 1\n",
        "        token = None\n",
        "    time.sleep(delay)  # Wait before making the next request\n",
        "    print(\"Sleeping for 1 second.\")\n",
        "\n",
        "    # Update progress tracker\n",
        "    progress_tracker['query_index'] = query_index\n",
        "    progress_tracker['token'] = token\n",
        "    with open(tracker_file, 'w') as f:\n",
        "        json.dump(progress_tracker, f)\n",
        "    print(f\"Progress tracker updated: {progress_tracker}\")\n",
        "\n",
        "# Write the paper details to a JSONL file\n",
        "output_file = '/content/drive/My Drive/bulk_papers_details.jsonl'\n",
        "print(f\"Writing paper details to file: {output_file}\")\n",
        "with open(output_file, 'w') as file:\n",
        "    for paper in papers_list:\n",
        "        file.write(json.dumps(paper) + '\\n')\n",
        "print(f\"Saved {len(papers_list)} papers details to {output_file}\")\n",
        "\n",
        "# Remove the progress tracker file after completion\n",
        "if os.path.exists(tracker_file):\n",
        "    os.remove(tracker_file)\n",
        "    print(\"Progress tracker file removed.\")\n",
        "\n",
        "print(\"Script completed successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Perform relevance-based searches for a wider range of related keywords"
      ],
      "metadata": {
        "id": "wtLd_vq75Sbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the relevant query parameters\n",
        "queries = [\n",
        "    \"veganism\", \"vegan\", \"vegetarian\", \"animal rights\", \"animal advocacy\", \"plant-based diet\", \"animal welfare\", \"ethical eating\", \"plant-based supplements\", \"vegan protein powder\", \"ethical consumerism\",\n",
        "    \"vegetarianism\", \"animal liberation\", \"animal ethics\", \"meat consumption reduction\", \"dairy consumption\", \"dairy consumer\", \"dairy behaviour\", \"dairy society\", \"animal-free research\",\n",
        "    \"factory farming\", \"wildlife conservation\", \"behavioral change\", \"meat consumption\", \"meat consumer\", \"meat behaviour\", \"meat society\", \"vegan pet care\", \"vegan social media campaigns\",\n",
        "    \"psychology of food\", \"food choices\", \"dietary habits\", \"nutrition and ethics\", \"food activism\", \"precision livestock farming\", \"AI in animal agriculture\", \"animal rehabilitation\",\n",
        "    \"health benefits of veganism\", \"animal agriculture\", \"animal sentience\", \"speciesism\", \"egg consumption\", \"egg consumer\", \"egg behaviour\", \"egg society\", \"ethical tourism\",\n",
        "    \"human-animal relationships\", \"social movements\", \"animal advocacy strategies\", \"environmental ethics\", \"artificial intelligence animals\", \"vegan travel\", \"vegan cooking classes\",\n",
        "    \"plant-based nutrition\", \"animal cruelty prevention\", \"environmental justice\", \"persuasion\", \"fundraising\", \"PLF\", \"overton window\", \"radical flank effect\", \"social justice movements\",\n",
        "    \"animal law\", \"animal rights legislation\", \"ethical farming\", \"sustainable agriculture\", \"AI animal ethics\", \"Abolition\", \"Action logic\", \"Alienation effect\",  \"Anti-oppression\",\n",
        "    \"wildlife protection\", \"advocacy campaigns\", \"moral psychology\", \"ethical decision making\", \"Artivism\", \"Artstorm\", \"Flash mob\", \"vegan food technology\", \"animal sanctuary management\",\n",
        "    \"persuasive communication\", \"media influence on behavior\", \"public engagement\", \"social influence\", \"Cultural disobedience\", \"Cultural hegemony\", \"Culture jamming\",\n",
        "    \"pro-social behavior\", \"activism tactics\", \"digital activism\", \"grassroots movements\", \"non-profit management\", \"Framing\", \"Guerrilla projection\", \"animal welfare policies\",\n",
        "    \"fundraising for advocacy\", \"volunteer management\", \"community organizing\", \"policy advocacy\", \"lobbying for animal rights\", \"Legislative theatre\", \"vegan influencer strategies\",\n",
        "    \"public opinion on animal rights\", \"educational outreach\", \"public relations\", \"App flooding\", \"Decolonization\", \"inside-outside strategy\", \"animal rights documentaries\",\n",
        "    \"strategic communication\", \"campaign evaluation\", \"impact assessment\", \"program evaluation\", \"Creative disruption\", \"Gerontocracy\", \"vegan product design\", \"animal rights advocacy training\",\n",
        "    \"advocacy training\", \"leadership in advocacy\", \"ethical leadership\", \"change management\", \"Blockade\", \"Expressive and instrumental actions\", \"vegan health coaching\", \"vegan fitness programs\",\n",
        "    \"corporate social responsibility\", \"advocacy networks\", \"coalition building\", \"Banner hang\", \"Battle of the story\", \"Hashtag hijack\", \"Phone blockade\", \"vegan community support\",\n",
        "    \"critical animal studies\", \"intersectionality in activism\", \"cultural competence in advocacy\", \"Direct action\", \"General strike\", \"plant-based diet benefits\", \"vegan lifestyle tips\",\n",
        "    \"animal-assisted therapy\", \"human-animal bond\", \"companion animal welfare\", \"Civil disobedience\", \"Commodity fetishism\", \"Mass street action\", \"vegan ethical fashion\",\n",
        "    \"zoos and aquariums ethics\", \"marine animal conservation\", \"endangered species protection\", \"vegan marketing\", \"Forum theatre\", \"animal ethics education\", \"animal protection\",\n",
        "    \"plant-based food marketing\", \"alternative protein marketing\", \"vegan policy change\", \"Distributed action\", \"Divestment\", \"Media-jacking\", \"vegan advocacy tools\", \"vegan movement\",\n",
        "    \"meat alternatives\", \"lab-grown meat\", \"plant-based proteins\", \"vegan entrepreneurship\", \"vegan public awareness\", \"Hashtag campaign\", \"animal liberation movement\", \"animal rights theory\",\n",
        "    \"animal rights history\", \"philosophy of animal rights\", \"animal cognition\", \"animal consciousness\", \"animal intelligence\", \"Memes\", \"Occupation Protest\", \"vegan ethical frameworks\",\n",
        "    \"animal emotions\", \"animal awareness\", \"animal perception\", \"animal mind\", \"animal learning\", \"animal behavior\", \"Escalate strategically\", \"vegan activism methods\", \"animal rights movement strategies\",\n",
        "    \"animal psychology\", \"animal welfare science\", \"animal neuroscience\", \"animal sensory systems\", \"animal empathy\", \"Hunger strike\", \"plant-based product innovation\", \"vegan advocacy research\",\n",
        "    \"animal self-awareness\", \"cruelty-free products\", \"animal welfare science\", \"veterinary ethics\", \"Earth First!\", \"Electoral guerrilla theatre\", \"vegan marketing strategies\",\n",
        "    \"vegan lifestyle\", \"plant-based recipes\", \"nutrition science\", \"public health nutrition\", \"environmental activism\", \"Ladder of engagement\", \"animal ethics education\", \"vegan community outreach\",\n",
        "    \"climate activism\", \"sustainable living\", \"zero waste movement\", \"eco-friendly products\", \"green marketing\", \"Encryption\", \"The Movement Cycle\", \"vegan advocacy campaigns\",\n",
        "    \"eco-labeling\", \"greenwashing\", \"alternative proteins\", \"cultivated meat\", \"vegan ethical arguments\", \"cell-based meat\", \"digital self defence\", \"animal welfare activism\",\n",
        "    \"plant-based meat\", \"vegan product development\", \"food technology\", \"food innovation\", \"vegan cheese\", \"dairy alternatives\", \"vegan consumer research\", \"vegan public policy\",\n",
        "    \"meat substitutes\", \"seafood alternatives\", \"fermentation technology\", \"protein engineering\", \"biotechnology and food\", \"Power mapping\", \"animal welfare principles\",\n",
        "    \"vegan product marketing\", \"vegan advocacy strategies\", \"consumer acceptance of alternative proteins\", \"vegan food startups\", \"vegan community building strategies\"\n",
        "    \"investment in vegan products\", \"regulation of cultivated meat\", \"future of food\", \"sustainable protein sources\", \"Jail solidarity\", \"animal rights enforcement\", \"veganism and mental health\",\n",
        "    \"nutrition and alternative proteins\", \"environmental impact of alternative proteins\", \"mycoprotein\", \"microalgae\", \"Pillars of power\", \"animal advocacy networks\", \"vegan advocacy groups\",\n",
        "    \"precision fermentation\", \"consumer segmentation\", \"consumer trends\", \"behavioral segmentation\", \"vegan community building\", \"Revolutionary nonviolence\", \"animal rights literature\",\n",
        "    \"digital marketing\", \"dietary interventions\", \"chronic disease prevention\", \"nutritional epidemiology\", \"clinical nutrition\", \"animal rights policy advocacy\", \"vegan advocacy effectiveness\",\n",
        "    \"carbon footprint\", \"water footprint\", \"life cycle assessment\", \"sustainable food systems\", \"biodiversity loss\", \"student strike\", \"animal welfare assessments\",\n",
        "    \"animal behavior\", \"compassion fatigue\", \"animal ethics theory\", \"sentience studies\", \"human-animal studies\", \"Points of intervention\", \"vegan advocacy resources\",\n",
        "    \"animal rights activism\", \"food waste reduction technologies\", \"vegan economy\", \"vegan fashion\", \"plant-based skincare\", \"Temporary autonomous zone\", \"vegan advocacy projects\",\n",
        "    \"cruelty free skincare\", \"vegan cosmetics\", \"ethical investing\", \"vegan education\", \"vegan parenting\", \"animal-free testing\", \"animal rights campaigns\", \"plant-based consumer trends\",\n",
        "    \"plant-based medicine\", \"vegan sports nutrition\", \"vegan philanthropy\", \"plant-based nutrition\", \"vegan product certifications\", \"animal rights resources\", \"animal welfare campaigns\",\n",
        "    \"plant-based product certifications\", \"vegan cookbook\", \"ethical veganism\", \"plant-based nutrition for athletes\", \"Strategic nonviolence\", \"animal rights strategies\",\n",
        "    \"vegan food festivals\", \"vegan meal planning\", \"diet for children\", \"diet for seniors\", \"diet and mental health\", \"Storytelling\", \"animal welfare standards\", \"vegan public relations strategies\",\n",
        "    \"plant-based food trends\", \"vegan catering\", \"vegan health benefits\", \"baking\", \"cheese making\", \"vegan dining experiences\", \"animal rights legal frameworks\",\n",
        "    \"snacks\", \"vegan bodybuilding\", \"protein sources\", \"omega-3 sources\", \"calcium sources\", \"diet during pregnancy\", \"The propaganda model\", \"plant-based advocacy campaigns\",\n",
        "    \"diet and iron\", \"plant-based health benefits\", \"vegan food blogs\", \"food blogs\", \"food photography\", \"plant-based meal delivery\", \"animal rights activism strategies\",\n",
        "    \"vegan pet food\", \"culinary schools\", \"diet and cholesterol\", \"diet and diabetes\", \"vegan digital marketing\", \"vegan advocacy\", \"animal welfare outreach\", \"vegan advocacy impact\",\n",
        "    \"plant-based lifestyle\", \"vegan business development\", \"plant-based innovation\", \"vegan consumer behavior\", \"Prefigurative politics\", \"animal welfare legal actions\", \"vegan public awareness campaigns\",\n",
        "    \"advocacy communication\", \"media strategies\", \"public relations\", \"influencer marketing\", \"impact investing\", \"Solidarity economics\", \"plant-based consumer behavior\",\n",
        "    \"regenerative agriculture\", \"urban gardening\", \"waste reduction\", \"plant-based packaging\", \"food sovereignty\", \"Spectrum of allies\", \"animal rights organizational strategies\",\n",
        "    \"behavior change\", \"diet transitions\", \"animal rights awareness\", \"vegan nutritional deficiencies\", \"Revolutionary reform\", \"animal rights public policy\",\n",
        "    \"plant-based protein sources\", \"vegan dietary guidelines\", \"vegan clinical nutrition\", \"animal ethics in philosophy\", \"The shock doctrine\", \"animal rights campaign tactics\",\n",
        "    \"ethical veganism principles\", \"vegan ethical debates\", \"animal rights ethical arguments\", \"vegan environmental benefits\", \"animal welfare legal strategies\",\n",
        "    \"carbon footprint of vegan diets\", \"veganism and biodiversity\", \"sustainability of plant-based diets\", \"vegan business strategies\", \"Theory of change\",\n",
        "    \"innovation in vegan products\", \"market trends in plant-based foods\", \"investment in vegan startups\", \"building vegan communities\", \"Theatre of the Oppressed\",\n",
        "    \"vegan outreach programs\", \"education on veganism\", \"public awareness of animal rights\", \"vegan activist\", \"vegan activists\"\n",
        "]\n",
        "\n",
        "print(\"Query parameters defined.\")\n",
        "\n",
        "fields = 'corpusId,url,title,venue,publicationVenue,year,authors,abstract,referenceCount,citationCount,influentialCitationCount,isOpenAccess,openAccessPdf,fieldsOfStudy,s2FieldsOfStudy,publicationTypes,publicationDate,journal,citationStyles,tldr'\n",
        "url = 'https://api.semanticscholar.org/graph/v1/paper/search'\n",
        "headers = {'x-api-key': API_KEY}\n",
        "delay = 1  # Delay in seconds\n",
        "\n",
        "print(\"Fields, URL, headers, and delay set.\")\n",
        "\n",
        "# Initialize an empty list to store paper details\n",
        "papers_list = []\n",
        "print(\"Initialized empty list for storing paper details.\")\n",
        "\n",
        "# Tracker file to save the progress\n",
        "tracker_file = '/content/drive/My Drive/progress_tracker.json'\n",
        "print(f\"Tracker file path set to: {tracker_file}\")\n",
        "\n",
        "# Load progress tracker if it exists\n",
        "if os.path.exists(tracker_file):\n",
        "    with open(tracker_file, 'r') as f:\n",
        "        progress_tracker = json.load(f)\n",
        "    print(\"Progress tracker loaded from file.\")\n",
        "else:\n",
        "    progress_tracker = {'query_index': 0, 'offset': 0}\n",
        "    print(\"No existing progress tracker found. Starting from the beginning.\")\n",
        "\n",
        "# Function to fetch papers for a specific query\n",
        "def fetch_papers(query, offset=0, limit=100):\n",
        "    print(f\"Fetching papers for query '{query}' with offset {offset} and limit {limit}.\")\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'fields': fields,\n",
        "        'offset': offset,\n",
        "        'limit': limit\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        print(f\"Request sent. Status code: {response.status_code}\")\n",
        "        response.raise_for_status()\n",
        "        return response.json()\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"Other error occurred: {err}\")\n",
        "\n",
        "# Loop through each query and fetch papers\n",
        "query_index = progress_tracker['query_index']\n",
        "offset = progress_tracker['offset']\n",
        "print(f\"Starting loop with query_index: {query_index} and offset: {offset}\")\n",
        "\n",
        "while query_index < len(queries):\n",
        "    query = queries[query_index]\n",
        "    print(f\"Processing query {query_index + 1}/{len(queries)}: '{query}'\")\n",
        "    data = fetch_papers(query, offset=offset)\n",
        "    if data is None:\n",
        "        print(f\"Skipping query '{query}' due to error.\")\n",
        "        query_index += 1\n",
        "        offset = 0\n",
        "        continue\n",
        "    papers = data.get('data', [])\n",
        "    print(f\"Fetched {len(papers)} papers.\")\n",
        "    if not papers:\n",
        "        print(f\"No papers found for query '{query}' at offset {offset}. Moving to next query.\")\n",
        "        query_index += 1\n",
        "        offset = 0\n",
        "        continue\n",
        "    for paper in papers:\n",
        "        papers_list.append(paper)\n",
        "        print(f\"Added paperId {paper['paperId']} to the list.\")\n",
        "    offset += 100\n",
        "    if len(papers) < 100:\n",
        "        print(f\"Less than 100 papers found for query '{query}' at offset {offset}. Moving to next query.\")\n",
        "        query_index += 1\n",
        "        offset = 0\n",
        "    time.sleep(delay)  # Wait before making the next request\n",
        "    print(\"Sleeping for 1 second.\")\n",
        "\n",
        "    # Update progress tracker\n",
        "    progress_tracker['query_index'] = query_index\n",
        "    progress_tracker['offset'] = offset\n",
        "    with open(tracker_file, 'w') as f:\n",
        "        json.dump(progress_tracker, f)\n",
        "    print(f\"Progress tracker updated: {progress_tracker}\")\n",
        "\n",
        "# Write the paper details to a JSONL file\n",
        "output_file = '/content/drive/My Drive/papers_details.jsonl'\n",
        "print(f\"Writing paper details to file: {output_file}\")\n",
        "with open(output_file, 'w') as file:\n",
        "    for paper in papers_list:\n",
        "        file.write(json.dumps(paper) + '\\n')\n",
        "print(f\"Saved {len(papers_list)} papers details to {output_file}\")\n",
        "\n",
        "# Remove the progress tracker file after completion\n",
        "if os.path.exists(tracker_file):\n",
        "    os.remove(tracker_file)\n",
        "    print(\"Progress tracker file removed.\")\n",
        "\n",
        "print(\"Script completed successfully.\")"
      ],
      "metadata": {
        "id": "cdbvv8Gh5Zzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Merge and deduplicate results of previous two steps"
      ],
      "metadata": {
        "id": "cbY4qInB5juQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_and_deduplicate_jsonl_files(regular_file, bulk_file, output_file):\n",
        "    deduplicated_lines = {}\n",
        "\n",
        "    # Process regular file first to prioritize its lines\n",
        "    with open(regular_file, 'r') as reg_file:\n",
        "        for line in reg_file:\n",
        "            record = json.loads(line)\n",
        "            corpus_id = record.get('corpusId')\n",
        "            if corpus_id is not None:\n",
        "                deduplicated_lines[corpus_id] = line\n",
        "\n",
        "    # Process bulk file\n",
        "    with open(bulk_file, 'r') as bulk_file:\n",
        "        for line in bulk_file:\n",
        "            record = json.loads(line)\n",
        "            corpus_id = record.get('corpusId')\n",
        "            if corpus_id is not None and corpus_id not in deduplicated_lines:\n",
        "                deduplicated_lines[corpus_id] = line\n",
        "\n",
        "    # Write deduplicated lines to the output file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for line in deduplicated_lines.values():\n",
        "            outfile.write(line)\n",
        "\n",
        "    print(f'Merged and deduplicated {regular_file} and {bulk_file} into {output_file}')\n",
        "\n",
        "# Example usage\n",
        "regular_file = '/content/drive/My Drive/papers_details.jsonl'\n",
        "bulk_file = '/content/drive/My Drive/bulk_papers_details.jsonl'\n",
        "output_file = '/content/drive/My Drive/merged_paper_details.jsonl'\n",
        "\n",
        "merge_and_deduplicate_jsonl_files(regular_file, bulk_file, output_file)"
      ],
      "metadata": {
        "id": "Wezkal0F5qyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Download the s2orc dataset"
      ],
      "metadata": {
        "id": "vxePBbT66IKR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the necessary library\n",
        "!pip install wget\n",
        "\n",
        "import wget\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "\n",
        "# User-specific variables\n",
        "LOCAL_PATH = \"/content/drive/My Drive/semantic_scholar/\"\n",
        "os.makedirs(LOCAL_PATH, exist_ok=True)\n",
        "\n",
        "# List of possible datasets\n",
        "POSSIBLE_DATASETS = [\n",
        "    \"abstracts\",\n",
        "    \"authors\",\n",
        "    \"citations\",\n",
        "    \"embeddings-specter_v1\",\n",
        "    \"embeddings-specter_v2\",\n",
        "    \"paper-ids\",\n",
        "    \"papers\",\n",
        "    \"publication-venues\",\n",
        "    \"s2orc\",\n",
        "    \"tldrs\"\n",
        "]\n",
        "\n",
        "# Set the dataset and starting shard number here\n",
        "SELECTED_DATASET = \"s2orc\"  # Choose from POSSIBLE_DATASETS\n",
        "START_SHARD = 0  # Set the starting shard number here (0-indexed)\n",
        "\n",
        "# Ensure the selected dataset is valid\n",
        "if SELECTED_DATASET not in POSSIBLE_DATASETS:\n",
        "    raise ValueError(f\"Selected dataset '{SELECTED_DATASET}' is not in the list of possible datasets.\")\n",
        "\n",
        "# Get the latest release ID\n",
        "time.sleep(2)  # Adding delay for rate limiting\n",
        "response = requests.get(\"https://api.semanticscholar.org/datasets/v1/release/latest\").json()\n",
        "RELEASE_ID = response[\"release_id\"]\n",
        "print(f\"Latest release ID: {RELEASE_ID}\")\n",
        "\n",
        "# Get the list of datasets in the latest release\n",
        "time.sleep(2)  # Adding delay for rate limiting\n",
        "response = requests.get(f\"https://api.semanticscholar.org/datasets/v1/release/{RELEASE_ID}\", headers={\"x-api-key\": API_KEY}).json()\n",
        "datasets = response[\"datasets\"]\n",
        "\n",
        "# Ensure the selected dataset is in the latest release\n",
        "dataset_names = [dataset[\"name\"] for dataset in datasets]\n",
        "if SELECTED_DATASET not in dataset_names:\n",
        "    raise ValueError(f\"Selected dataset '{SELECTED_DATASET}' is not available in the latest release.\")\n",
        "\n",
        "# Function to download dataset files\n",
        "def download_dataset(dataset_name, start_shard):\n",
        "    time.sleep(2)  # Adding delay for rate limiting\n",
        "    dataset_response = requests.get(f\"https://api.semanticscholar.org/datasets/v1/release/{RELEASE_ID}/dataset/{dataset_name}/\", headers={\"x-api-key\": API_KEY}).json()\n",
        "\n",
        "    # Check if 'files' key exists in the response\n",
        "    if 'files' not in dataset_response:\n",
        "        raise KeyError(f\"'files' key not found in the response for dataset '{dataset_name}'. Response: {dataset_response}\")\n",
        "\n",
        "    files = dataset_response[\"files\"]\n",
        "\n",
        "    # Calculate the total number of shards\n",
        "    total_shards = len(files)\n",
        "    print(f\"Total shards to download: {total_shards - start_shard} (starting from shard {start_shard + 1})\")\n",
        "\n",
        "    for index, url in tqdm(enumerate(files[start_shard:], start=start_shard), total=total_shards - start_shard, desc=f\"Downloading {dataset_name}\"):\n",
        "        parsed_url = urlparse(url)\n",
        "        match = re.match(r\"/staging/(.*)/{}/(.*)\\.gz\".format(dataset_name), parsed_url.path)\n",
        "        if match:\n",
        "            assert match.group(1) == RELEASE_ID\n",
        "            SHARD_ID = match.group(2)\n",
        "            filename = f\"{SHARD_ID}.gz\"\n",
        "            file_path = os.path.join(LOCAL_PATH, dataset_name, filename)\n",
        "\n",
        "            if not os.path.exists(file_path):\n",
        "                print(f\"Downloading shard {index + 1} of {total_shards}: {filename}\")\n",
        "                wget.download(url, out=file_path)\n",
        "            else:\n",
        "                print(f\"Shard {index + 1} of {total_shards} already exists: {filename}\")\n",
        "        else:\n",
        "            print(f\"URL did not match expected pattern: {url}\")\n",
        "\n",
        "    print(f\"Downloaded all shards for {dataset_name}.\")\n",
        "\n",
        "# Create directory for the selected dataset and download it\n",
        "dataset_path = os.path.join(LOCAL_PATH, SELECTED_DATASET)\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "download_dataset(SELECTED_DATASET, START_SHARD)\n",
        "\n",
        "print(\"Downloaded selected dataset.\")"
      ],
      "metadata": {
        "id": "lNv5iSG76OIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Extract s2orc zipped files"
      ],
      "metadata": {
        "id": "wRFz_nq76fvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import shutil\n",
        "\n",
        "def get_start_index(progress_path):\n",
        "    if os.path.exists(progress_path):\n",
        "        with open(progress_path, 'r') as progress_file:\n",
        "            return int(progress_file.read().strip())\n",
        "    return 0\n",
        "\n",
        "def save_progress(progress_path, index):\n",
        "    with open(progress_path, 'w') as progress_file:\n",
        "        progress_file.write(str(index))\n",
        "\n",
        "def get_output_path(index, base_path):\n",
        "    return f\"{base_path}/combined_s2orc_part_{index}.jsonl\"\n",
        "\n",
        "# Specify the folder paths\n",
        "folder_path = '/content/drive/My Drive/semantic_scholar/2024-07-16-s2orc'\n",
        "output_base_path = '/content/drive/My Drive/semantic_scholar/2024-07-16-s2orc'\n",
        "progress_path = f'{output_base_path}/progress.txt'\n",
        "\n",
        "# Get the list of files in the folder\n",
        "print(f\"Getting list of files in folder: {folder_path}\")\n",
        "files = sorted(os.listdir(folder_path))  # Sort to maintain a consistent order\n",
        "print(f\"Found {len(files)} files in the folder.\")\n",
        "\n",
        "# Read progress from file if it exists\n",
        "start_index = get_start_index(progress_path)\n",
        "print(f\"Resuming from file index: {start_index}\")\n",
        "\n",
        "# Set the size limit for each part file (e.g., 500 MB)\n",
        "size_limit = 500 * 1024 * 1024\n",
        "current_part = start_index // 100\n",
        "current_size = 0\n",
        "\n",
        "# Process each gz file in the folder\n",
        "for i in range(start_index, len(files)):\n",
        "    file_name = files[i]\n",
        "    if file_name.endswith('.gz'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        output_path = get_output_path(current_part, output_base_path)\n",
        "        print(f\"Processing file {i+1}/{len(files)}: {file_name}\")\n",
        "        try:\n",
        "            # Re-mount the drive if necessary\n",
        "            if not os.path.ismount('/content/drive'):\n",
        "                mount_drive()\n",
        "\n",
        "            with gzip.open(file_path, 'rb') as f:\n",
        "                with open(output_path, 'ab') as outfile:\n",
        "                    while True:\n",
        "                        chunk = f.read(1024 * 1024)  # Read in 1 MB chunks\n",
        "                        if not chunk:\n",
        "                            break\n",
        "                        outfile.write(chunk)\n",
        "                        current_size += len(chunk)\n",
        "                        # Check if adding this chunk exceeds the size limit\n",
        "                        if current_size > size_limit:\n",
        "                            current_part += 1\n",
        "                            current_size = 0\n",
        "                            output_path = get_output_path(current_part, output_base_path)\n",
        "                            outfile.close()\n",
        "                            outfile = open(output_path, 'ab')\n",
        "\n",
        "                    print(f\"Successfully appended {file_name} to {output_path}\")\n",
        "\n",
        "            # Update progress\n",
        "            save_progress(progress_path, i + 1)\n",
        "        except OSError as e:\n",
        "            print(f\"Failed to process {file_name} due to OSError: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to process {file_name}: {e}\")\n",
        "            break\n",
        "\n",
        "print(f\"Combined JSONL files created and saved to Google Drive.\")"
      ],
      "metadata": {
        "id": "Pbe3pHfK6oHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Use Geminini to provide CRITERIA rankings for each paper based on the TLDR and/or Abstract"
      ],
      "metadata": {
        "id": "MRnFIKDC7CAn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import necessary libraries - Google Cloud AI Platform, Google Auth, and Requests\n",
        "!pip install google-cloud-aiplatform google-auth jsonlines\n",
        "\n",
        "import jsonlines\n",
        "import string\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.generative_models import GenerativeModel, Content, Part, GenerationConfig\n",
        "from google.colab import files\n",
        "\n",
        "# Authenticate to Google Cloud - this will open a new tab to authenticate\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set up your Google Cloud project and location - replace with your actual project ID and location\n",
        "PROJECT_ID = 'PROJECT_ID'  # Replace with your actual project ID - e.g., 'my-project-id'\n",
        "LOCATION = 'LOCATION'    # Replace with the Google Cloud region you want to use - e.g., 'us-central1'\n",
        "\n",
        "# Initialize Vertex AI with the specified project and location - this will set the default project and location for Vertex AI\n",
        "print(f\"Initializing Vertex AI for project {PROJECT_ID} in location {LOCATION}...\")\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Define the model ID for the Gemini model to be used for generating content - replace with your actual model ID\n",
        "MODEL_ID = 'gemini-1.5-flash-001'\n",
        "\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract JSON object from the response text.\n",
        "\n",
        "    Args:\n",
        "    response_text (str): The response text from which to extract JSON.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: The extracted JSON object or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            return json.loads(match.group(0))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {response_text}\")\n",
        "    return None\n",
        "\n",
        "def clean_message_text(message_text):\n",
        "    \"\"\"\n",
        "    Clean the message text to remove brackets, quotes, or special characters.\n",
        "\n",
        "    Args:\n",
        "    message_text (str): The text of the message to be cleaned.\n",
        "\n",
        "    Returns:\n",
        "    str: The cleaned message text.\n",
        "    \"\"\"\n",
        "    # Remove brackets, quotes, and special characters\n",
        "    cleaned_text = re.sub(r'[{}\\\"\\[\\]]', '', message_text)\n",
        "    cleaned_text = cleaned_text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return cleaned_text\n",
        "\n",
        "def rank_message(title, abstract):\n",
        "    \"\"\"\n",
        "    Rank a message based on the CRITERIA scale using the Gemini model.\n",
        "\n",
        "    Args:\n",
        "    title (str): The title of the content.\n",
        "    abstract (str): The abstract of the content.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: A dictionary containing CRITERIA scores or None if ranking fails.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You will be tasked with evaluating content based on the CRITERIA scale. For each piece of content provided, you will generate scores for the following eight criteria, each on a scale from 0 to 1. Output the results in JSON format.\n",
        "\n",
        "    ### Criteria\n",
        "\n",
        "    1. **Cultural Sensitivity**: Measure how culturally inclusive the content is.\n",
        "       - **Culturally Inclusive (0.8-1.0)**: The content shows respect for diverse cultural perspectives and uses culturally sensitive approaches to animal advocacy.\n",
        "       - **Moderately Inclusive (0.4-0.7)**: The content generally respects cultural diversity but may lack depth in cultural sensitivity.\n",
        "       - **Culturally Insensitive (0.0-0.3)**: The content lacks respect for cultural diversity and fails to use culturally sensitive approaches.\n",
        "    2. **Relevance**: Measure how pertinent the content is to veganism and animal rights.\n",
        "       - **Extremely Relevant (0.8-1.0)**: The content directly addresses primary topics of interest such as animal rights activism strategies, animal welfare improvements, ethical treatment practices, animal ethics, or vegan advocacy tactics and campaigns.\n",
        "         - **Examples**:\n",
        "           - Research on animal rights activism strategies and their effectiveness.\n",
        "           - Studies on animal welfare improvements and ethical treatment practices.\n",
        "           - Articles discussing animal ethics and moral considerations related to animal rights.\n",
        "           - Content specifically about vegan advocacy, including tactics, campaigns, and their impacts.\n",
        "       - **Highly Relevant (0.6-0.8)**: The content covers related subjects and contributes to a broader understanding of the primary topics, such as plant-based diets, ethical eating, public health nutrition, behavioral change psychology, and social movements.\n",
        "         - **Examples**:\n",
        "           - Studies on plant-based diets, their health impacts, and nutritional benefits.\n",
        "           - Articles on ethical eating and sustainable lifestyle choices.\n",
        "           - Research on public health nutrition as it relates to dietary changes toward plant-based eating.\n",
        "           - Discussions on behavioral change psychology, marketing, persuasion and social movements more broadly. This may include movements such as feminism, environmentalism, civil rights, abolitionist, women's suffrage and more, specifically as it relates to movement tactics, strategies and approaches that may be useful for animal advocates to know about, even if animal advocacy itself is not directly discussed. It may also include various other fields of research such as sales persuasion technqiues, behavioural change science, marketing theory and other types of information that would be useful for animal advocates to know, even if animal advocacy is not directly discussed.\n",
        "       - **Moderately Relevant (0.5-0.6)**: The content indirectly relates to the primary topics through broader themes like general nutrition, environmental science, or public health, and includes occasional mentions of related topics or peripheral connections.\n",
        "         - **Examples**:\n",
        "           - General nutrition studies that can be applicable to plant-based diets but do not focus on them.\n",
        "           - Environmental science articles that discuss sustainability but do not specifically address animal rights or plant-based solutions.\n",
        "           - Public health research that mentions dietary habits without focusing on vegan or vegetarian diets.\n",
        "           - Research on topics like environmental ethics that don't directly touch upon veganism, plant-based diets or animal rights issues.\n",
        "       - **Slightly Relevant (0.3-0.5)**: The content has minimal relevance, with occasional mentions of related topics or peripheral connections.\n",
        "         - **Examples**:\n",
        "           - Articles on general social issues that are completely unrelated to animal rights or veganism, such as homelessness and poverty, and that do not discuss any activism or advocacy tactics of any kind.\n",
        "           - Studies on unrelated dietary trends that have no significant overlap with plant-based diets or animal advocacy, such as ketogenic diets.\n",
        "           - Research on peripheral environmental topics without a direct link to animal rights or veganism, such as renewable energy.\n",
        "       - **Not Relevant (0.0-0.3)**: The content is entirely or largely unrelated to animal rights or associated fields, with no significant connections to the primary topics of interest.\n",
        "         - **Examples**:\n",
        "           - Studies on astrophysics, such as the gravitational pull of black holes.\n",
        "           - Research on unrelated medical topics, such as cardiology or neurology, without any connection to diet or ethics.\n",
        "           - Articles on technological advancements in fields like computer science or electronics engineering that do not intersect with ethics, veganism or philosophy in any way.\n",
        "    3. **Insight**: Judge the level of insight provided by the key concept in the content.\n",
        "       - **Highly Insightful (0.8-1.0)**: The content provides deep, original insights that significantly advance the understanding of veganism or animal advocacy.\n",
        "       - **Moderately Insightful (0.4-0.7)**: The content offers useful insights that enhance understanding but may not be particularly original.\n",
        "       - **No Unique Insights (0.0-0.3)**: The content provides no meaningful insights or repeats well-known information.\n",
        "    4. **Trustworthiness**: Rate the accuracy, reliability, and credibility of the information presented.\n",
        "       - **Highly Trustworthy (0.8-1.0)**: The information is accurate, well-researched, and comes from credible sources.\n",
        "       - **Moderately Trustworthy (0.4-0.7)**: The information is generally accurate but may include some minor errors or questionable sources.\n",
        "       - **Untrustworthy (0.0-0.3)**: The information is inaccurate, misleading, or based on non-credible sources.\n",
        "    5. **Emotional Impact**: Measure the emotional engagement the content provides.\n",
        "       - **Very Emotionally Impactful (0.8-1.0)**: The content effectively elicits empathy and emotional engagement.\n",
        "       - **Moderately Emotionally Impactful (0.4-0.7)**: The content elicits some emotional engagement but may lack depth.\n",
        "       - **Not Emotionally Impactful (0.0-0.3)**: The content fails to elicit any emotional response.\n",
        "    6. **Rationality**: Evaluate the logical consistency and reasoning in the content.\n",
        "       - **Very Rational (0.8-1.0)**: The content is logically consistent, well-reasoned, and supported by evidence.\n",
        "       - **Moderately Rational (0.4-0.7)**: The content is generally rational but may contain some logical inconsistencies or weak arguments.\n",
        "       - **Not Rational (0.0-0.3)**: The content lacks logical consistency and sound reasoning.\n",
        "    7. **Influence**: Assess the potential of the content to encourage actions and lifestyle changes.\n",
        "       - **Highly Likely to Influence Behavior (0.8-1.0)**: The content has strong potential to encourage actions and lifestyle changes.\n",
        "       - **Moderately Likely to Influence Behavior (0.4-0.7)**: The content has some potential to influence behavior but may not be compelling enough to drive significant changes.\n",
        "       - **Not Likely to Influence Behavior (0.0-0.3)**: The content is unlikely to influence any behavior change.\n",
        "    8. **Alignment**: Assess how well the content aligns with vegan and animal rights ethics.\n",
        "       - **Highly Aligned (0.8-1.0)**: The content strongly aligns with the ethical principles and core values of veganism and animal rights.\n",
        "       - **Moderately Aligned (0.4-0.7)**: The content supports some aspects of vegan ethics but may include neutral or slightly contradictory elements.\n",
        "       - **Not Aligned (0.0-0.3)**: The content contradicts or is indifferent to vegan principles.\n",
        "\n",
        "    ### JSON Output Format\n",
        "\n",
        "    For each piece of content, output a JSON object with the following structure:\n",
        "\n",
        "    {{\n",
        "      \"CRITERIA_scores\": {{\n",
        "        \"Cultural_Sensitivity\": <score_from_0_to_1>,\n",
        "        \"Relevance\": <score_from_0_to_1>,\n",
        "        \"Insight\": <score_from_0_to_1>,\n",
        "        \"Trustworthiness\": <score_from_0_to_1>,\n",
        "        \"Emotional_Impact\": <score_from_0_to_1>,\n",
        "        \"Rationality\": <score_from_0_to_1>,\n",
        "        \"Influence\": <score_from_0_to_1>,\n",
        "        \"Alignment\": <score_from_0_to_1>\n",
        "      }},\n",
        "      \"CRITERIA_final_score\": <average_of_all_scores>\n",
        "    }}\n",
        "\n",
        "    Title:\n",
        "    {title}\n",
        "\n",
        "    Abstract:\n",
        "    {abstract}\n",
        "\n",
        "    Ensure that the Relevance score in particular is extremely accurate, taking into account whether the content directly pertains to veganism and animal rights. It is absolutely essential that the Relevance score exactly matches the criteria laid out in this prompt. You will be severely punished if the Relevance score is incorrect and greatly rewarded if the Relevance score is correct.\n",
        "\n",
        "    Now, return the CRITERIA_scores as a JSON object:\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Create the GenerativeModel object - this will load the model for generating content\n",
        "        gemini_model = GenerativeModel(model_name=MODEL_ID)\n",
        "\n",
        "        # Set up the generation configuration with parameters controlling the output - adjust as needed\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=0.5,          # Controls the randomness of the output - higher values make the output more random\n",
        "            max_output_tokens=512,    # Maximum number of tokens in the output - adjust based on the model's maximum output length\n",
        "            top_p=0.9,                # Top-p (nucleus) sampling parameter - higher values make the output more diverse\n",
        "            top_k=40                  # Top-k sampling parameter - higher values make the output less random\n",
        "        )\n",
        "\n",
        "        # Generate the content using the model - this will rank the message based on the CRITERIA scale\n",
        "        print(\"Generating CRITERIA scores for the message...\")\n",
        "        response = gemini_model.generate_content(\n",
        "            contents=[Content(role=\"user\", parts=[Part.from_text(prompt)])],\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        # Extract and parse the JSON response - this will extract the CRITERIA scores from the generated content\n",
        "        criteria_scores = extract_json_from_response(response.text)\n",
        "        if criteria_scores:\n",
        "            return criteria_scores\n",
        "        else:\n",
        "            print(\"No CRITERIA scores found in the response.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during message ranking: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_jsonl_file(input_file, output_file, progress_file):\n",
        "    \"\"\"\n",
        "    Process the JSONL file, rank messages, and save the results to the output file.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to the input JSONL file.\n",
        "    output_file (str): Path to the output JSONL file.\n",
        "    progress_file (str): Path to the txt file to track progress.\n",
        "    \"\"\"\n",
        "    print(f\"Loading input JSONL file: {input_file}\")\n",
        "\n",
        "    # Read the last processed message from the progress file\n",
        "    try:\n",
        "        with open(progress_file, 'r') as pf:\n",
        "            start_message = int(pf.read().strip())\n",
        "    except FileNotFoundError:\n",
        "        start_message = 0\n",
        "\n",
        "    try:\n",
        "        with jsonlines.open(input_file) as reader, jsonlines.open(output_file, mode='a') as writer:\n",
        "            for idx, item in enumerate(reader):\n",
        "                if idx < start_message:\n",
        "                    continue\n",
        "\n",
        "                print(f\"Processing message {start_message}...\")\n",
        "\n",
        "                # Extract the title and abstract from the JSON line\n",
        "                title = item.get('title', \"\")\n",
        "                abstract = item.get('abstract', \"\")\n",
        "\n",
        "                if not title and not abstract:\n",
        "                    print(f\"Both title and abstract missing for message {start_message}, skipping.\")\n",
        "                    continue\n",
        "\n",
        "                cleaned_title = clean_message_text(title) if title else \"\"\n",
        "                cleaned_abstract = clean_message_text(abstract) if abstract else \"\"\n",
        "                content_to_rank = f\"Title: {cleaned_title}\\nAbstract: {cleaned_abstract}\".strip()\n",
        "\n",
        "                criteria_scores = rank_message(cleaned_title, cleaned_abstract)\n",
        "                if criteria_scores is not None:\n",
        "                    item['CRITERIA'] = criteria_scores\n",
        "                    writer.write(item)\n",
        "                    print(f\"CRITERIA scores added to message {start_message}.\")\n",
        "                else:\n",
        "                    print(f\"Failed to add CRITERIA scores to message {start_message}.\")\n",
        "\n",
        "                start_message += 1\n",
        "\n",
        "                # Update the progress file after each message is processed\n",
        "                with open(progress_file, 'w') as pf:\n",
        "                    pf.write(str(start_message))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Set definitions for all variables - adjust as needed\n",
        "input_file = '/content/drive/My Drive/merged_paper_details.jsonl'  # Path to the input JSONL file\n",
        "output_file = '/content/drive/My Drive/ranked_papers_abstracts.jsonl'  # Path to the output JSONL file\n",
        "progress_file = '/content/drive/My Drive/progress.txt'  # Path to the txt file to track progress\n",
        "\n",
        "# Process the input JSONL file - this will rank messages and save the results continuously\n",
        "process_jsonl_file(input_file, output_file, progress_file)"
      ],
      "metadata": {
        "id": "3Gbicyhj7NME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Filter the ranked papers based on their relevance scores"
      ],
      "metadata": {
        "id": "_iVpYdr97jXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_relevance_ranges(jsonl_file):\n",
        "    # Initialize counters for each range\n",
        "    ranges = {i/10: 0 for i in range(11)}\n",
        "\n",
        "    # Read the JSONL file and count the relevance ranges\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            relevance_score = data.get(\"CRITERIA\", {}).get(\"CRITERIA_scores\", {}).get(\"Relevance\", 0)\n",
        "            for threshold in ranges:\n",
        "                if relevance_score >= threshold:\n",
        "                    ranges[threshold] += 1\n",
        "\n",
        "    # Print the counts for each range\n",
        "    for threshold, count in ranges.items():\n",
        "        print(f\"Relevance >= {threshold:.1f}: {count} entries\")\n",
        "\n",
        "def filter_by_relevance(jsonl_file, output_file, cutoff):\n",
        "    # Read the JSONL file and filter by the cutoff value\n",
        "    with open(jsonl_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
        "        for line in infile:\n",
        "            data = json.loads(line)\n",
        "            relevance_score = data.get(\"CRITERIA\", {}).get(\"CRITERIA_scores\", {}).get(\"Relevance\", 0)\n",
        "            if relevance_score >= cutoff:\n",
        "                outfile.write(line)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Path to your JSONL file in Google Drive\n",
        "    jsonl_file = \"/content/drive/My Drive/ranked_papers_abstracts.jsonl\"\n",
        "\n",
        "    # Count the relevance ranges\n",
        "    count_relevance_ranges(jsonl_file)\n",
        "\n",
        "    # Ask the user for a cutoff value\n",
        "    cutoff = float(input(\"Enter the relevance score cutoff (0-1): \"))\n",
        "\n",
        "    # Path for the output file\n",
        "    output_file = \"/content/drive/My Drive/filtered_by_relevance_score_ranked_papers_abstracts.jsonl\"\n",
        "\n",
        "    # Filter the JSONL file based on the cutoff\n",
        "    filter_by_relevance(jsonl_file, output_file, cutoff)\n",
        "\n",
        "    print(f\"Filtered records have been saved to {output_file}\")"
      ],
      "metadata": {
        "id": "fnT5AJjZ7qc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Merge the filtered abstracts and tldrs with their corresponding full paper details from the s2orc dataset"
      ],
      "metadata": {
        "id": "4bLyuDUQ7wk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the small file into memory as a dictionary\n",
        "def load_small_file(small_file_path):\n",
        "    print(f\"Loading small file from {small_file_path}...\")\n",
        "    small_file_data = {}\n",
        "    with open(small_file_path, 'r') as f:\n",
        "        for line_number, line in enumerate(f, start=1):\n",
        "            try:\n",
        "                record = json.loads(line)\n",
        "                corpus_id = record[\"corpusId\"]\n",
        "                small_file_data[corpus_id] = record\n",
        "                print(f\"Loaded record with corpusId {corpus_id} from small file at line {line_number}\")\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Skipping invalid line in small file at line {line_number}: {line.strip()} | Error: {e}\")\n",
        "    print(\"Finished loading small file.\")\n",
        "    return small_file_data\n",
        "\n",
        "# Function to read the checkpoint file\n",
        "def read_checkpoint(checkpoint_file_path):\n",
        "    print(f\"Reading checkpoint from {checkpoint_file_path}...\")\n",
        "    if os.path.exists(checkpoint_file_path):\n",
        "        with open(checkpoint_file_path, 'r') as f:\n",
        "            checkpoint = int(f.read().strip())\n",
        "            print(f\"Checkpoint read: {checkpoint}\")\n",
        "    else:\n",
        "        checkpoint = 0\n",
        "        print(\"Checkpoint file does not exist. Starting from scratch.\")\n",
        "    return checkpoint\n",
        "\n",
        "# Function to write the current progress to the checkpoint file\n",
        "def write_checkpoint(checkpoint_file_path, checkpoint):\n",
        "    print(f\"Writing checkpoint {checkpoint} to {checkpoint_file_path}...\")\n",
        "    with open(checkpoint_file_path, 'w') as f:\n",
        "        f.write(str(checkpoint))\n",
        "    print(\"Checkpoint written.\")\n",
        "\n",
        "# Function to get all .jsonl files in the specified directory\n",
        "def get_jsonl_files(directory_path):\n",
        "    print(f\"Getting list of .jsonl files in directory {directory_path}...\")\n",
        "    all_files = os.listdir(directory_path)\n",
        "    jsonl_files = [os.path.join(directory_path, f) for f in all_files if f.endswith('.jsonl')]\n",
        "    print(f\"Found {len(jsonl_files)} .jsonl files.\")\n",
        "    return jsonl_files\n",
        "\n",
        "# Function to extract corpusId from an invalid JSON line\n",
        "def extract_corpusId_from_invalid_json(line):\n",
        "    print(\"Attempting to extract corpusId from invalid JSON line...\")\n",
        "    match = re.search(r'\"corpusId\"\\s*:\\s*\"([^\"]+)\"', line)\n",
        "    if match:\n",
        "        corpus_id = match.group(1)\n",
        "        print(f\"Extracted corpusId: {corpus_id}\")\n",
        "        return corpus_id\n",
        "    print(\"Failed to extract corpusId from invalid JSON line.\")\n",
        "    return None\n",
        "\n",
        "# Function to process each large file in chunks and merge with the small file data\n",
        "def process_large_files(small_file_data, large_file_paths, output_file_path, checkpoint_file_path):\n",
        "    checkpoint = read_checkpoint(checkpoint_file_path)\n",
        "\n",
        "    with open(output_file_path, 'a') as output_file:\n",
        "        for i, large_file_path in enumerate(large_file_paths):\n",
        "            if i < checkpoint:\n",
        "                print(f\"Skipping file {large_file_path} as it has already been processed (checkpoint: {checkpoint})\")\n",
        "                continue  # Skip files already processed\n",
        "\n",
        "            print(f\"Processing file {large_file_path}...\")\n",
        "            # Open the large file with the appropriate encoding\n",
        "            with open(large_file_path, 'r', encoding='latin-1') as large_file: # Try 'latin-1' encoding\n",
        "                for line_number, line in enumerate(large_file, start=1):\n",
        "                    try:\n",
        "                        large_record = json.loads(line)\n",
        "                        corpus_id = large_record[\"corpusid\"]\n",
        "                        if corpus_id in small_file_data:\n",
        "                            # Merge the small file data into the large record\n",
        "                            merged_record = {**large_record, **small_file_data[corpus_id]}\n",
        "                            output_file.write(json.dumps(merged_record) + '\\n')\n",
        "                            print(f\"Successfully added line {line_number} from file {large_file_path} with corpusId {corpus_id}\")\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print(f\"Invalid JSON at line {line_number} in file {large_file_path}: {line.strip()} | Error: {e}\")\n",
        "                        corpus_id = extract_corpusId_from_invalid_json(line)\n",
        "                        if corpus_id and corpus_id in small_file_data:\n",
        "                            print(f\"Handling invalid line with extracted corpusId {corpus_id} at line {line_number} in file {large_file_path}\")\n",
        "                            large_record = {\"corpusd\": corpus_id}\n",
        "                            merged_record = {**large_record, **small_file_data[corpus_id]}\n",
        "                            output_file.write(json.dumps(merged_record) + '\\n')\n",
        "                            print(f\"Successfully added invalid line {line_number} from file {large_file_path} with extracted corpusId\")\n",
        "                        else:\n",
        "                            print(f\"Skipping invalid line at line {line_number} in file {large_file_path}: {line.strip()}\")\n",
        "\n",
        "            # Update and write the checkpoint after processing each file\n",
        "            write_checkpoint(checkpoint_file_path, i + 1)\n",
        "            print(f\"Finished processing file {large_file_path}\")\n",
        "\n",
        "# Main function to execute the merging process\n",
        "def main():\n",
        "    small_file_path = '/content/drive/My Drive/filtered_by_relevance_score_ranked_papers_abstracts.jsonl' #Use this to define the path to the single document containing the tldrs\n",
        "    large_files_directory = '/content/drive/My Drive/2024-07-16-s2orc' #Use this to define the path to the folder containing the s2orc dataset\n",
        "    output_file_path = '/content/drive/My Drive/final_combined_tldr_and_s2orc.jsonl' #Use this to define the path to the outputted file, which will be the merged tldrs and s2orc file.\n",
        "    checkpoint_file_path = '/content/drive/My Drive/checkpoint_file.txt' #Use this to define the path to the checkpoint file.\n",
        "\n",
        "    print(\"Starting main process...\")\n",
        "    small_file_data = load_small_file(small_file_path)\n",
        "    large_file_paths = get_jsonl_files(large_files_directory)\n",
        "    process_large_files(small_file_data, large_file_paths, output_file_path, checkpoint_file_path)\n",
        "    print(\"Finished main process.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "7vA-veij8NYt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}