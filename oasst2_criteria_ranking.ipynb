{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcP9fHPM_yL4"
      },
      "source": [
        "This script is designed to augment the [Open Assistant/oasst2](huggingface.co/datasets/OpenAssistant/oasst2) dataset by adding a CRITERIA score, which helps filter out irrelevant messages unrelated to veganism or animal rights.\n",
        "\n",
        "The filtered dataset can then be used either as seed data for further human feedback collection or directly for model training.\n",
        "\n",
        "The CRITERIA score evaluates eight key aspects of the content:\n",
        "\n",
        "*   Cultural Sensitivity (C): Respect for diverse cultural perspectives in animal advocacy.\n",
        "*   Relevance (R): Pertinence of the content to veganism and animal rights.\n",
        "*  Insight (I): Level of original insights provided by the content.\n",
        "*  Trustworthiness (T): Accuracy, reliability, and credibility of the information.\n",
        "*  Emotional Impact (E): Effectiveness in eliciting empathy and emotional\n",
        " engagement.\n",
        "*  Rationality (R): Logical consistency and reasoning in the content.\n",
        "* Influence (I): Potential of the content to encourage actions and lifestyle changes.\n",
        "* Alignment (A): Consistency with vegan and animal rights ethics.\n",
        "\n",
        "Running this script requires an account with permissions to a Google Cloud project that has Vertex AI's API enabled and also requires the oaast2 dataset being placed inside that account's Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yCvWmV2AX1S",
        "outputId": "37196e45-0f35-4856-a622-16ea7d5b3722"
      },
      "outputs": [],
      "source": [
        "# Install and import necessary libraries - Google Cloud AI Platform, Google Auth, and Requests\n",
        "!pip install google-cloud-aiplatform google-auth requests\n",
        "\n",
        "import json\n",
        "import re\n",
        "import google.auth\n",
        "from google.colab import auth\n",
        "from google.cloud import aiplatform\n",
        "from google.colab import drive\n",
        "from vertexai.generative_models import GenerativeModel, Content, Part, GenerationConfig\n",
        "from google.colab import files\n",
        "\n",
        "# Authenticate to Google Cloud - this will open a new tab to authenticate\n",
        "print(\"Authenticating to Google Cloud...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Mount Google Drive to access files stored in the drive - this will open a new tab to authenticate\n",
        "print(\"Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up your Google Cloud project and location - replace with your actual project ID and location\n",
        "PROJECT_ID = 'PROJECT_ID'  # Replace with your actual project ID - e.g., 'my-project-id'\n",
        "LOCATION = 'LOCATION'    # Replace with the Google Cloud region you want to use - e.g., 'us-central1'\n",
        "\n",
        "# Initialize Vertex AI with the specified project and location - this will set the default project and location for Vertex AI\n",
        "print(f\"Initializing Vertex AI for project {PROJECT_ID} in location {LOCATION}...\")\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# Define the model ID for the Gemini model to be used for generating content - replace with your actual model ID\n",
        "MODEL_ID = 'gemini-1.5-flash-001'\n",
        "\n",
        "def extract_json_from_response(response_text):\n",
        "    \"\"\"\n",
        "    Extract JSON object from the response text.\n",
        "\n",
        "    Args:\n",
        "    response_text (str): The response text from which to extract JSON.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: The extracted JSON object or None if extraction fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.search(r'{.*}', response_text, re.DOTALL)\n",
        "        if match:\n",
        "            return json.loads(match.group(0))\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Failed to parse JSON response: {response_text}\")\n",
        "    return None\n",
        "\n",
        "def rank_message(message_text):\n",
        "    \"\"\"\n",
        "    Rank a message based on the CRITERIA scale using the Gemini model.\n",
        "\n",
        "    Args:\n",
        "    message_text (str): The text of the message to be ranked.\n",
        "\n",
        "    Returns:\n",
        "    dict or None: A dictionary containing CRITERIA scores or None if ranking fails.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You will be tasked with evaluating content based on the CRITERIA scale. For each piece of content provided, you will generate scores for the following eight criteria, each on a scale from 0 to 1. Output the results in JSON format.\n",
        "\n",
        "    ### Criteria\n",
        "\n",
        "    1. **Cultural Sensitivity**: Measure how culturally inclusive the content is.\n",
        "       - **Culturally Inclusive (0.8-1.0)**: The content shows respect for diverse cultural perspectives and uses culturally sensitive approaches to animal advocacy.\n",
        "       - **Moderately Inclusive (0.4-0.7)**: The content generally respects cultural diversity but may lack depth in cultural sensitivity.\n",
        "       - **Culturally Insensitive (0.0-0.3)**: The content lacks respect for cultural diversity and fails to use culturally sensitive approaches.\n",
        "    2. **Relevance**: Measure how pertinent the content is to veganism and animal rights.\n",
        "       - **Highly Relevant (0.8-1.0)**: The content is directly related to veganism, animal rights, vegan lifestyle, plant-based diets, animal welfare, ethical treatment of animals, or advocacy for animal rights.\n",
        "       - **Moderately Relevant (0.4-0.7)**: The content indirectly relates to veganism and animal rights through broader ethical, dietary, or sustainability discussions.\n",
        "       - **Not Relevant (0.0-0.3)**: The content is unrelated to veganism or animal rights.\n",
        "    3. **Insight**: Judge the level of insight provided by the key concept in the content.\n",
        "       - **Highly Insightful (0.8-1.0)**: The content provides deep, original insights that significantly advance the understanding of veganism or animal advocacy.\n",
        "       - **Moderately Insightful (0.4-0.7)**: The content offers useful insights that enhance understanding but may not be particularly original.\n",
        "       - **No Unique Insights (0.0-0.3)**: The content provides no meaningful insights or repeats well-known information.\n",
        "    4. **Trustworthiness**: Rate the accuracy, reliability, and credibility of the information presented.\n",
        "       - **Highly Trustworthy (0.8-1.0)**: The information is accurate, well-researched, and comes from credible sources.\n",
        "       - **Moderately Trustworthy (0.4-0.7)**: The information is generally accurate but may include some minor errors or questionable sources.\n",
        "       - **Untrustworthy (0.0-0.3)**: The information is inaccurate, misleading, or based on non-credible sources.\n",
        "    5. **Emotional Impact**: Measure the emotional engagement the content provides.\n",
        "       - **Very Emotionally Impactful (0.8-1.0)**: The content effectively elicits empathy and emotional engagement.\n",
        "       - **Moderately Emotionally Impactful (0.4-0.7)**: The content elicits some emotional engagement but may lack depth.\n",
        "       - **Not Emotionally Impactful (0.0-0.3)**: The content fails to elicit any emotional response.\n",
        "    6. **Rationality**: Evaluate the logical consistency and reasoning in the content.\n",
        "       - **Very Rational (0.8-1.0)**: The content is logically consistent, well-reasoned, and supported by evidence.\n",
        "       - **Moderately Rational (0.4-0.7)**: The content is generally rational but may contain some logical inconsistencies or weak arguments.\n",
        "       - **Not Rational (0.0-0.3)**: The content lacks logical consistency and sound reasoning.\n",
        "    7. **Influence**: Assess the potential of the content to encourage actions and lifestyle changes.\n",
        "       - **Highly Likely to Influence Behavior (0.8-1.0)**: The content has strong potential to encourage actions and lifestyle changes.\n",
        "       - **Moderately Likely to Influence Behavior (0.4-0.7)**: The content has some potential to influence behavior but may not be compelling enough to drive significant changes.\n",
        "       - **Not Likely to Influence Behavior (0.0-0.3)**: The content is unlikely to influence any behavior change.\n",
        "    8. **Alignment**: Assess how well the content aligns with vegan and animal rights ethics.\n",
        "       - **Highly Aligned (0.8-1.0)**: The content strongly aligns with the ethical principles and core values of veganism and animal rights.\n",
        "       - **Moderately Aligned (0.4-0.7)**: The content supports some aspects of vegan ethics but may include neutral or slightly contradictory elements.\n",
        "       - **Not Aligned (0.0-0.3)**: The content contradicts or is indifferent to vegan principles.\n",
        "\n",
        "    ### JSON Output Format\n",
        "\n",
        "    For each piece of content, output a JSON object with the following structure:\n",
        "\n",
        "    {{\n",
        "      \"CRITERIA_scores\": {{\n",
        "        \"Cultural_Sensitivity\": <score_from_0_to_1>,\n",
        "        \"Relevance\": <score_from_0_to_1>,\n",
        "        \"Insight\": <score_from_0_to_1>,\n",
        "        \"Trustworthiness\": <score_from_0_to_1>,\n",
        "        \"Emotional_Impact\": <score_from_0_to_1>,\n",
        "        \"Rationality\": <score_from_0_to_1>,\n",
        "        \"Influence\": <score_from_0_to_1>,\n",
        "        \"Alignment\": <score_from_0_to_1>\n",
        "      }},\n",
        "      \"CRITERIA_final_score\": <average_of_all_scores>\n",
        "    }}\n",
        "\n",
        "    Content:\n",
        "    {message_text}\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Create the GenerativeModel object - this will load the model for generating content\n",
        "        gemini_model = GenerativeModel(model_name=MODEL_ID)\n",
        "\n",
        "        # Set up the generation configuration with parameters controlling the output - adjust as needed\n",
        "        generation_config = GenerationConfig(\n",
        "            temperature=0.7,          # Controls the randomness of the output - higher values make the output more random\n",
        "            max_output_tokens=512,    # Maximum number of tokens in the output - adjust based on the model's maximum output length\n",
        "            top_p=0.9,                # Top-p (nucleus) sampling parameter - higher values make the output more diverse\n",
        "            top_k=40                  # Top-k sampling parameter - higher values make the output less random\n",
        "        )\n",
        "\n",
        "        # Generate the content using the model - this will rank the message based on the CRITERIA scale\n",
        "        print(\"Generating CRITERIA scores for the message...\")\n",
        "        response = gemini_model.generate_content(\n",
        "            contents=[Content(role=\"user\", parts=[Part.from_text(prompt)])],\n",
        "            generation_config=generation_config\n",
        "        )\n",
        "\n",
        "        # Extract and parse the JSON response - this will extract the CRITERIA scores from the generated content\n",
        "        criteria_scores = extract_json_from_response(response.text)\n",
        "        if criteria_scores:\n",
        "            return {\n",
        "                \"Cultural_Sensitivity\": criteria_scores[\"CRITERIA_scores\"][\"Cultural_Sensitivity\"],\n",
        "                \"Relevance\": criteria_scores[\"CRITERIA_scores\"][\"Relevance\"],\n",
        "                \"Insight\": criteria_scores[\"CRITERIA_scores\"][\"Insight\"],\n",
        "                \"Trustworthiness\": criteria_scores[\"CRITERIA_scores\"][\"Trustworthiness\"],\n",
        "                \"Emotional_Impact\": criteria_scores[\"CRITERIA_scores\"][\"Emotional_Impact\"],\n",
        "                \"Rationality\": criteria_scores[\"CRITERIA_scores\"][\"Rationality\"],\n",
        "                \"Influence\": criteria_scores[\"CRITERIA_scores\"][\"Influence\"],\n",
        "                \"Alignment\": criteria_scores[\"CRITERIA_scores\"][\"Alignment\"],\n",
        "                \"CRITERIA_final_score\": criteria_scores[\"CRITERIA_final_score\"]\n",
        "            }\n",
        "        else:\n",
        "            print(\"No CRITERIA scores found in the response.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during message ranking: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_file(input_file, output_file, max_messages, start_message):\n",
        "    \"\"\"\n",
        "    Process the input file, rank messages, and save the results to the output file.\n",
        "\n",
        "    Args:\n",
        "    input_file (str): Path to the input JSONL file.\n",
        "    output_file (str): Path to the output JSONL file.\n",
        "    max_messages (int): Maximum number of messages to process.\n",
        "    start_message (int): Starting message number for processing.\n",
        "    \"\"\"\n",
        "    print(f\"Processing file: {input_file}\")\n",
        "    message_count = 0\n",
        "\n",
        "    try:\n",
        "        with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'a', encoding='utf-8') as outfile:\n",
        "            # Skip lines until the start_message - useful for resuming processing\n",
        "            for _ in range(start_message - 1):\n",
        "                try:\n",
        "                    next(infile)\n",
        "                except StopIteration:\n",
        "                    print(\"Reached the end of file while skipping lines.\")\n",
        "                    return\n",
        "\n",
        "            for line in infile:\n",
        "                if message_count >= max_messages:\n",
        "                    print(\"Reached the maximum number of messages to process.\")\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    message_tree = json.loads(line)\n",
        "                except json.JSONDecodeError:\n",
        "                    print(\"Error decoding JSON, skipping line.\")\n",
        "                    continue\n",
        "\n",
        "                def process_message(message):\n",
        "                    \"\"\"\n",
        "                    Recursively process a message and its replies.\n",
        "\n",
        "                    Args:\n",
        "                    message (dict): The message to process.\n",
        "                    \"\"\"\n",
        "                    nonlocal message_count\n",
        "                    if message_count >= max_messages:\n",
        "                        return\n",
        "\n",
        "                    print(f\"Processing message {start_message + message_count}...\")\n",
        "                    criteria_scores = rank_message(message['text'])\n",
        "                    if criteria_scores is not None:\n",
        "                        message['CRITERIA'] = criteria_scores\n",
        "                        print(f\"CRITERIA scores added to message {start_message + message_count}.\")\n",
        "                    else:\n",
        "                        print(f\"Failed to add CRITERIA scores to message {start_message + message_count}.\")\n",
        "\n",
        "                    for reply in message.get('replies', []):\n",
        "                        process_message(reply)\n",
        "\n",
        "                    message_count += 1\n",
        "\n",
        "                # Process the main message and its replies - this will rank the message and its replies\n",
        "                process_message(message_tree['prompt'])\n",
        "                for reply in message_tree['prompt'].get('replies', []):\n",
        "                    process_message(reply)\n",
        "\n",
        "                # Write the processed message to the output file - this will save the message with CRITERIA scores\n",
        "                json.dump(message_tree, outfile, ensure_ascii=False)\n",
        "                outfile.write('\\n')\n",
        "                print(f\"Message {start_message + message_count} written to output file.\")\n",
        "\n",
        "            # Final save and download at the end of processing - this will save the final progress\n",
        "            print(\"Final save...\")\n",
        "            files.download(output_file)\n",
        "            print(f\"File {output_file} downloaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Set definitions for all variables - adjust as needed\n",
        "input_file = 'INPUT_FILE_PATH'  # Path to the input JSONL file - replace with your file path\n",
        "output_file = 'OUTPUT_FILE_PATH'  # Path to the output JSONL file - replace with the name you want for the output file\n",
        "max_messages = 5000  # Set the maximum number of messages to process - recommend starting with 5,000 or less to avoid timeouts\n",
        "start_message = 1  # Starting message number for processing - adjust as needed to begin script from a specific message\n",
        "\n",
        "# Process the file with a message limit - this will rank messages and save the results\n",
        "process_file(input_file, output_file, max_messages, start_message)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
